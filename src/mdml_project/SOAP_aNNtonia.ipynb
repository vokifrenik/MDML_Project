{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fingerprint: Coulomb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Coulomb import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import KFold  \n",
    "import joblib  # For saving and loading scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=251)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14280\\293079480.py\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Normalize the target (hform)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtarget_scaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# You can use StandardScaler if needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'to_numpy'"
     ]
    }
   ],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Normalize the target (hform)\n",
    "target_scaler = MinMaxScaler()  # You can use StandardScaler if needed\n",
    "y_train = target_scaler.fit_transform(y_train.reshape(-1, 1) if isinstance(y_train, np.ndarray) else y_train.to_numpy().reshape(-1, 1))\n",
    "y_test = target_scaler.transform(y_test.reshape(-1, 1) if isinstance(y_test, np.ndarray) else y_test.to_numpy().reshape(-1, 1))\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(RegressionNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)  # Increased neurons\n",
    "        self.bn1 = nn.BatchNorm1d(256)  # Batch normalization\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.fc5 = nn.Linear(32, 1)\n",
    "        self.dropout = nn.Dropout(p=0.2)  # Dropout to reduce overfitting\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.fc1(x)))  # LeakyReLU activation\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define cross-validation training loop with train_test_split\n",
    "def cross_val_train(model_class, X_train, y_train, epochs, k_folds):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train)):\n",
    "        print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
    "\n",
    "        # Use train_test_split to split the fold's training data\n",
    "        X_fold_train, X_val, y_fold_train, y_val = train_test_split(\n",
    "            X_train[train_idx], y_train[train_idx], test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        # Initialize model, optimizer, scheduler\n",
    "        model = model_class(X_fold_train.shape[1])\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_fold_train)\n",
    "            loss = criterion(outputs, y_fold_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Step the learning rate scheduler\n",
    "            scheduler.step()\n",
    "\n",
    "            # Evaluation phase\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(X_val)\n",
    "                val_loss = criterion(val_outputs, y_val)\n",
    "\n",
    "            # Convert MSE to RMSE for better interpretability\n",
    "            rmse = torch.sqrt(loss).item()\n",
    "            val_rmse = torch.sqrt(val_loss).item()\n",
    "\n",
    "            # Print RMSE every 50 epochs\n",
    "            if (epoch + 1) % 500 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], RMSE: {rmse:.4f}, Val RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "        # Store final validation loss for the fold\n",
    "        fold_results.append(val_loss.item())\n",
    "\n",
    "        # Save the model state if it's the best so far\n",
    "        if val_loss.item() < best_val_loss:\n",
    "            best_val_loss = val_loss.item()\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    # Print overall results\n",
    "    print(\"\\nCross-Validation Results:\")\n",
    "    print(f\"Fold Losses: {fold_results}\")\n",
    "    print(f\"Mean Validation Loss: {np.mean(fold_results):.4f}\")\n",
    "    print(f\"Standard Deviation: {np.std(fold_results):.4f}\")\n",
    "\n",
    "    # Save the best model state\n",
    "    torch.save(best_model_state, \"best_model.pth\")\n",
    "    print(\"Best model saved as 'best_model.pth'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "Epoch [500/1000], RMSE: 0.5241, Val RMSE: 0.5638\n",
      "Epoch [1000/1000], RMSE: 0.5227, Val RMSE: 0.5634\n",
      "\n",
      "Fold 2/5\n",
      "Epoch [500/1000], RMSE: 0.5261, Val RMSE: 0.5430\n",
      "Epoch [1000/1000], RMSE: 0.5210, Val RMSE: 0.5430\n",
      "\n",
      "Fold 3/5\n",
      "Epoch [500/1000], RMSE: 0.5301, Val RMSE: 0.5532\n",
      "Epoch [1000/1000], RMSE: 0.5248, Val RMSE: 0.5532\n",
      "\n",
      "Fold 4/5\n",
      "Epoch [500/1000], RMSE: 0.5324, Val RMSE: 0.5851\n",
      "Epoch [1000/1000], RMSE: 0.5331, Val RMSE: 0.5853\n",
      "\n",
      "Fold 5/5\n",
      "Epoch [500/1000], RMSE: 0.5216, Val RMSE: 0.5683\n",
      "Epoch [1000/1000], RMSE: 0.5223, Val RMSE: 0.5684\n",
      "\n",
      "Cross-Validation Results:\n",
      "Fold Losses: [0.3174709677696228, 0.2948089838027954, 0.3059844374656677, 0.3425616919994354, 0.3230755925178528]\n",
      "Mean Validation Loss: 0.3168\n",
      "Standard Deviation: 0.0161\n",
      "Best model saved as 'best_model.pth'.\n"
     ]
    }
   ],
   "source": [
    "# Initialize loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Perform cross-validation\n",
    "cross_val_train(RegressionNN, X_train, y_train, epochs=1000, k_folds=5)\n",
    "\n",
    "# Save the model\n",
    "# Example usage: torch.save(model.state_dict(), \"enhanced_regression_model.pth\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Antonia\\AppData\\Local\\Temp\\ipykernel_14280\\4051752618.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], RMSE: 0.5125\n",
      "Epoch [200/1000], RMSE: 0.4957\n",
      "Epoch [300/1000], RMSE: 0.4945\n",
      "Epoch [400/1000], RMSE: 0.4949\n",
      "Epoch [500/1000], RMSE: 0.4920\n",
      "Epoch [600/1000], RMSE: 0.4915\n",
      "Epoch [700/1000], RMSE: 0.4924\n",
      "Epoch [800/1000], RMSE: 0.4915\n",
      "Epoch [900/1000], RMSE: 0.4926\n",
      "Epoch [1000/1000], RMSE: 0.4901\n",
      "Training on full dataset completed.\n",
      "\n",
      "Test RMSE: 0.5578\n",
      "Final model saved as 'final_model.pth'.\n"
     ]
    }
   ],
   "source": [
    "# Load the best model and train it on the full training set\n",
    "def train_on_full_data(model_class, X_train, y_train, X_test, y_test, criterion, epochs=100):\n",
    "    model = model_class(X_train.shape[1])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "    # Load the best model state\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            rmse = torch.sqrt(loss).item()\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}], RMSE: {rmse:.4f}\")\n",
    "\n",
    "    print(\"Training on full dataset completed.\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test)\n",
    "        test_loss = criterion(test_outputs, y_test)\n",
    "        test_rmse = torch.sqrt(test_loss).item()\n",
    "        print(f\"\\nTest RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "    # Save the final model\n",
    "    torch.save(model.state_dict(), \"final_model.pth\")\n",
    "    print(\"Final model saved as 'final_model.pth'.\")\n",
    "\n",
    "# Train the best model on the full training set and evaluate on test set\n",
    "train_on_full_data(RegressionNN, X_train, y_train, X_test, y_test, criterion, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0.11333410441875458]\n",
    "[0.11041968315839767]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MDMLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
