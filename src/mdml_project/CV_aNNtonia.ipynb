{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fingerprint: Coulomb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Coulomb import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import KFold  \n",
    "import joblib  # For saving and loading scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=251)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Normalize the target (hform)\n",
    "#target_scaler = MinMaxScaler()  # You can use StandardScaler if needed\n",
    "#y_train = target_scaler.fit_transform(y_train.reshape(-1, 1) if isinstance(y_train, np.ndarray) else y_train.to_numpy().reshape(-1, 1))\n",
    "#y_test = target_scaler.transform(y_test.reshape(-1, 1) if isinstance(y_test, np.ndarray) else y_test.to_numpy().reshape(-1, 1))\n",
    "\n",
    "# Ensure y_train and y_test are properly converted to NumPy arrays\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "\n",
    "# Convert y_train and y_test to NumPy arrays if they are Series or other objects\n",
    "if isinstance(y_train, pd.Series):\n",
    "    y_train = y_train.to_numpy()\n",
    "\n",
    "if isinstance(y_test, pd.Series):\n",
    "    y_test = y_test.to_numpy()\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Add dimension\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)  # Add dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(RegressionNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)  # Increased neurons\n",
    "        self.bn1 = nn.BatchNorm1d(256)  # Batch normalization\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.fc5 = nn.Linear(32, 1)\n",
    "        self.dropout = nn.Dropout(p=0.2)  # Dropout to reduce overfitting\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))  # LeakyReLU activation\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_train(model_class, X_train, y_train, epochs, k_folds, patience=50):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "    best_overall_val_loss = float('inf')\n",
    "    best_overall_model_state = None\n",
    "\n",
    "    if not isinstance(X_train, torch.Tensor):\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    if not isinstance(y_train, torch.Tensor):\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train)):\n",
    "        print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
    "\n",
    "        X_fold_train = X_train[train_idx]\n",
    "        y_fold_train = y_train[train_idx]\n",
    "        X_val = X_train[val_idx]\n",
    "        y_val = y_train[val_idx]\n",
    "\n",
    "        model = model_class(X_train.shape[1])\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20, verbose=True)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        best_fold_val_loss = float('inf')\n",
    "        best_fold_model_state = None\n",
    "        patience_counter = 0\n",
    "        best_epoch_rmse = float('inf')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_fold_train)\n",
    "            loss = criterion(outputs, y_fold_train)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(X_val)\n",
    "                val_loss = criterion(val_outputs, y_val)\n",
    "\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            if val_loss < best_fold_val_loss:\n",
    "                best_fold_val_loss = val_loss\n",
    "                best_fold_model_state = model.state_dict()\n",
    "                patience_counter = 0\n",
    "                best_epoch_rmse = torch.sqrt(val_loss).item()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            train_rmse = torch.sqrt(loss).item()\n",
    "            val_rmse = torch.sqrt(val_loss).item()\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Train RMSE: {train_rmse:.4f}, Val RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "        fold_results.append(best_epoch_rmse)\n",
    "\n",
    "        if best_fold_val_loss < best_overall_val_loss:\n",
    "            best_overall_val_loss = best_fold_val_loss\n",
    "            best_overall_model_state = best_fold_model_state.copy()\n",
    "            print(f\"New best model found in fold {fold + 1}\")\n",
    "\n",
    "    print(\"\\nCross-Validation Results:\")\n",
    "    print(f\"Fold RMSEs: {[f'{rmse:.4f}' for rmse in fold_results]}\")\n",
    "    print(f\"Mean RMSE: {np.mean(fold_results):.4f}\")\n",
    "    print(f\"Standard Deviation: {np.std(fold_results):.4f}\")\n",
    "\n",
    "    # Save only the model state\n",
    "    torch.save(best_overall_model_state, \"best_model_state.pth\")\n",
    "    print(\"Best model state saved as 'best_model_state.pth'.\")\n",
    "\n",
    "    return best_overall_model_state, fold_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/3\n",
      "New best model found in fold 1\n",
      "\n",
      "Fold 2/3\n",
      "\n",
      "Fold 3/3\n",
      "\n",
      "Cross-Validation Results:\n",
      "Fold RMSEs: ['0.5863', '0.5991', '0.5887']\n",
      "Mean RMSE: 0.5914\n",
      "Standard Deviation: 0.0056\n",
      "Best model state saved as 'best_model_state.pth'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('fc1.weight',\n",
       "               tensor([[-1.3519e-03, -6.1236e-03, -1.7919e-03,  ...,  9.0031e-03,\n",
       "                        -4.8412e-03, -9.8319e-03],\n",
       "                       [-1.6846e-02, -1.0922e-02,  2.7495e-02,  ..., -7.7153e-03,\n",
       "                        -1.7040e-03,  1.1466e-03],\n",
       "                       [-4.9724e-03,  7.7742e-03,  3.3037e-03,  ...,  7.3299e-04,\n",
       "                        -3.2818e-03, -2.8968e-03],\n",
       "                       ...,\n",
       "                       [-7.7775e-03,  2.0717e-02, -1.7368e-03,  ..., -9.8619e-03,\n",
       "                        -1.6995e-02, -2.6100e-03],\n",
       "                       [-1.5317e-03,  2.6319e-02,  2.7501e-03,  ...,  2.5388e-02,\n",
       "                         9.2079e-03, -1.0185e-02],\n",
       "                       [ 1.6772e-02, -1.2983e-02, -2.6280e-02,  ..., -7.0048e-03,\n",
       "                        -4.2891e-05,  2.4481e-03]])),\n",
       "              ('fc1.bias',\n",
       "               tensor([-8.2353e-03,  4.9515e-03, -6.6919e-04, -1.4332e-03,  1.9930e-03,\n",
       "                        7.3351e-05, -4.1145e-04,  8.6341e-04,  2.5563e-04, -6.8776e-03,\n",
       "                       -1.9789e-03,  3.9883e-03,  7.8776e-03, -8.0026e-03, -1.2209e-03,\n",
       "                        3.0016e-04, -3.3899e-03, -8.0209e-04,  1.5071e-04,  2.5016e-04,\n",
       "                       -1.1605e-04,  8.5761e-04,  5.2095e-04,  2.7851e-04, -9.0035e-04,\n",
       "                       -1.8150e-03,  1.4166e-03, -9.0036e-03,  1.2183e-03, -7.4006e-04,\n",
       "                       -1.9162e-03, -1.5425e-03, -1.5607e-04,  3.4110e-04, -1.9364e-03,\n",
       "                       -1.9992e-03, -1.5928e-03,  8.8486e-03,  4.7275e-04, -1.5409e-03,\n",
       "                       -2.5922e-03, -1.2718e-04,  1.2907e-03, -6.1823e-03,  1.4615e-03,\n",
       "                        2.6380e-03,  1.6192e-05,  1.2660e-03,  1.2107e-04,  7.9934e-04,\n",
       "                       -1.0556e-04,  5.3916e-04, -1.0283e-05, -1.7477e-05, -1.7585e-03,\n",
       "                        1.1364e-03, -8.2962e-04, -1.6592e-03,  1.4881e-03,  5.3787e-04,\n",
       "                        5.4250e-03,  5.2621e-03,  4.8216e-04, -8.5177e-05,  1.5671e-04,\n",
       "                        6.3098e-04, -1.9452e-03,  1.1722e-04,  5.5498e-04, -4.5183e-04,\n",
       "                       -8.6286e-04, -4.5587e-04,  3.0395e-05,  7.6542e-04,  7.1812e-03,\n",
       "                       -1.7408e-03, -1.2837e-03,  9.2332e-04,  3.4994e-04,  5.7513e-05,\n",
       "                        4.0428e-04, -1.6368e-03, -1.4392e-03, -5.7455e-04, -8.4767e-04,\n",
       "                       -8.2791e-05, -3.0115e-04, -1.0289e-04, -2.9968e-03, -1.9949e-03,\n",
       "                       -8.6690e-04,  1.6264e-04,  1.7462e-03, -3.3565e-04, -1.1621e-03,\n",
       "                        8.3774e-04,  7.1476e-04, -6.9013e-05,  2.3979e-06,  7.9231e-03,\n",
       "                       -2.7123e-04, -4.8034e-05, -1.9771e-03,  5.8358e-03,  6.2630e-04,\n",
       "                        5.6131e-04, -3.8768e-03,  2.2639e-04, -3.3801e-05,  3.1609e-04,\n",
       "                       -6.2439e-03, -3.5757e-04,  3.2162e-03,  1.6992e-03,  7.7204e-06,\n",
       "                        1.1904e-03,  2.9145e-03, -2.1568e-05, -1.9144e-03,  1.9619e-03,\n",
       "                       -2.0010e-03,  1.7853e-03,  2.9793e-03, -1.7899e-03, -1.4519e-03,\n",
       "                       -8.2349e-03,  6.6729e-04,  4.7427e-04, -4.6534e-05, -3.9003e-04,\n",
       "                       -8.5396e-04, -1.2314e-03,  8.2862e-06, -3.1421e-05, -2.5530e-05,\n",
       "                       -3.1269e-05,  2.0017e-03, -1.2030e-04,  8.4065e-04, -4.1592e-03,\n",
       "                        5.8075e-03, -1.0672e-03,  3.6491e-04,  1.1089e-04,  1.3930e-03,\n",
       "                        3.6137e-04,  4.0046e-04, -2.5480e-04,  1.5078e-03, -1.8826e-03,\n",
       "                        1.1759e-03, -4.7626e-03, -8.9961e-04, -4.6580e-04, -4.2121e-04,\n",
       "                        1.9149e-04,  7.4675e-04, -3.4411e-04,  8.7339e-04, -7.3210e-04,\n",
       "                        7.2810e-04, -2.0004e-03,  3.0984e-03,  7.8648e-03,  6.9259e-04,\n",
       "                       -3.1489e-03, -1.8750e-04,  4.4640e-04, -7.3471e-03,  1.7449e-04,\n",
       "                        2.0017e-03, -1.4182e-04,  7.9080e-03, -7.8187e-05, -1.3925e-04,\n",
       "                       -1.0678e-04, -4.2252e-04, -1.9180e-03, -1.2383e-03, -2.6281e-03,\n",
       "                       -8.4975e-04,  8.9372e-05,  1.9681e-03,  1.6241e-03, -7.0645e-03,\n",
       "                        5.8881e-03,  5.8926e-04, -7.2178e-03, -4.3493e-04,  9.5441e-04,\n",
       "                       -4.5718e-04,  1.9389e-03,  5.6242e-03, -1.8191e-04, -1.9514e-03,\n",
       "                        5.8393e-03,  1.6500e-03, -5.7691e-03,  8.1725e-04, -8.2989e-03,\n",
       "                       -4.8665e-03, -2.8331e-05,  5.2240e-03, -1.1339e-04,  2.2082e-04,\n",
       "                       -5.8247e-04,  3.0212e-03,  6.1002e-03, -1.7873e-04, -1.0568e-03,\n",
       "                        1.7405e-04, -3.5634e-03,  8.4689e-04, -7.2615e-04,  3.0944e-03,\n",
       "                        9.3985e-05,  7.2277e-05,  2.7065e-03,  5.5945e-04, -7.6789e-03,\n",
       "                       -1.8351e-03, -2.0026e-03,  3.6785e-04, -4.5627e-04,  1.0594e-04,\n",
       "                        3.0010e-04,  1.6665e-03,  7.2332e-03,  1.9773e-03,  1.9243e-03,\n",
       "                        4.5350e-04, -1.8556e-03,  1.1754e-04, -3.6352e-03,  1.7880e-04,\n",
       "                       -1.5546e-03,  7.1465e-04, -1.6146e-03,  1.8212e-03,  1.2883e-03,\n",
       "                       -2.5323e-05,  1.7038e-03,  1.2517e-04,  1.4543e-04, -1.8464e-03,\n",
       "                        1.7201e-03,  3.7081e-04,  1.9252e-03, -7.1948e-03,  4.2682e-03,\n",
       "                        1.2738e-05, -5.0457e-04, -6.2640e-04, -6.0041e-04, -9.6432e-05,\n",
       "                       -7.8004e-03])),\n",
       "              ('bn1.weight',\n",
       "               tensor([0.9504, 0.9503, 0.9503, 0.9504, 0.9505, 0.9504, 0.9504, 0.9504, 0.9503,\n",
       "                       0.9503, 0.9502, 0.9506, 0.9505, 0.9504, 0.9507, 0.9505, 0.9500, 0.9506,\n",
       "                       0.9503, 0.9506, 0.9504, 0.9502, 0.9502, 0.9504, 0.9502, 0.9508, 0.9509,\n",
       "                       0.9508, 0.9505, 0.9503, 0.9502, 0.9504, 0.9503, 0.9505, 0.9502, 0.9505,\n",
       "                       0.9507, 0.9504, 0.9507, 0.9502, 0.9506, 0.9504, 0.9502, 0.9504, 0.9504,\n",
       "                       0.9503, 0.9503, 0.9504, 0.9503, 0.9501, 0.9506, 0.9505, 0.9505, 0.9504,\n",
       "                       0.9503, 0.9503, 0.9504, 0.9502, 0.9504, 0.9505, 0.9505, 0.9504, 0.9503,\n",
       "                       0.9502, 0.9503, 0.9505, 0.9499, 0.9503, 0.9504, 0.9507, 0.9502, 0.9501,\n",
       "                       0.9502, 0.9505, 0.9497, 0.9504, 0.9496, 0.9503, 0.9504, 0.9502, 0.9502,\n",
       "                       0.9503, 0.9502, 0.9505, 0.9501, 0.9506, 0.9506, 0.9497, 0.9504, 0.9504,\n",
       "                       0.9504, 0.9503, 0.9502, 0.9505, 0.9503, 0.9503, 0.9502, 0.9504, 0.9505,\n",
       "                       0.9500, 0.9501, 0.9507, 0.9504, 0.9506, 0.9502, 0.9504, 0.9503, 0.9504,\n",
       "                       0.9505, 0.9500, 0.9505, 0.9499, 0.9502, 0.9502, 0.9503, 0.9508, 0.9502,\n",
       "                       0.9503, 0.9504, 0.9502, 0.9506, 0.9502, 0.9504, 0.9507, 0.9504, 0.9504,\n",
       "                       0.9501, 0.9501, 0.9504, 0.9503, 0.9503, 0.9502, 0.9506, 0.9505, 0.9502,\n",
       "                       0.9505, 0.9499, 0.9502, 0.9508, 0.9503, 0.9504, 0.9503, 0.9503, 0.9501,\n",
       "                       0.9505, 0.9504, 0.9502, 0.9503, 0.9504, 0.9503, 0.9501, 0.9502, 0.9505,\n",
       "                       0.9504, 0.9501, 0.9503, 0.9503, 0.9505, 0.9502, 0.9508, 0.9503, 0.9504,\n",
       "                       0.9504, 0.9502, 0.9503, 0.9502, 0.9502, 0.9503, 0.9503, 0.9504, 0.9503,\n",
       "                       0.9505, 0.9504, 0.9501, 0.9506, 0.9506, 0.9500, 0.9500, 0.9502, 0.9502,\n",
       "                       0.9506, 0.9501, 0.9503, 0.9493, 0.9504, 0.9502, 0.9503, 0.9511, 0.9508,\n",
       "                       0.9502, 0.9502, 0.9504, 0.9506, 0.9503, 0.9506, 0.9507, 0.9502, 0.9511,\n",
       "                       0.9502, 0.9505, 0.9502, 0.9504, 0.9504, 0.9500, 0.9503, 0.9499, 0.9503,\n",
       "                       0.9505, 0.9503, 0.9498, 0.9502, 0.9500, 0.9504, 0.9504, 0.9503, 0.9505,\n",
       "                       0.9503, 0.9503, 0.9505, 0.9502, 0.9504, 0.9503, 0.9500, 0.9497, 0.9504,\n",
       "                       0.9505, 0.9502, 0.9504, 0.9501, 0.9504, 0.9507, 0.9501, 0.9502, 0.9504,\n",
       "                       0.9501, 0.9507, 0.9505, 0.9504, 0.9505, 0.9506, 0.9502, 0.9504, 0.9502,\n",
       "                       0.9504, 0.9502, 0.9500, 0.9505, 0.9503, 0.9504, 0.9503, 0.9501, 0.9504,\n",
       "                       0.9502, 0.9502, 0.9502, 0.9504])),\n",
       "              ('bn1.bias',\n",
       "               tensor([ 4.2539e-03, -5.5779e-03, -6.3223e-03,  1.4754e-03,  2.8665e-03,\n",
       "                       -3.2578e-03, -1.2767e-02,  7.9686e-04, -6.7537e-04,  1.6574e-03,\n",
       "                       -6.9868e-04, -5.1354e-03, -2.8741e-03, -4.9378e-03, -5.4083e-03,\n",
       "                        5.2299e-03,  7.0700e-03,  6.2109e-03, -2.6957e-04, -1.1157e-02,\n",
       "                       -5.9392e-05,  8.8023e-03, -6.1490e-03,  2.3620e-03,  2.4554e-03,\n",
       "                        2.0701e-05, -9.7743e-03,  2.0142e-03, -2.0130e-02,  8.0145e-03,\n",
       "                       -2.9548e-03,  4.4128e-03,  7.5832e-03,  5.3212e-03, -6.5676e-03,\n",
       "                        8.6427e-04, -4.7694e-03, -1.1473e-02, -7.3527e-04, -1.0328e-02,\n",
       "                       -4.8903e-03, -1.7208e-03, -1.2416e-02, -4.6119e-03, -2.0924e-02,\n",
       "                       -4.1291e-03,  4.5168e-03,  8.2553e-03, -8.3592e-03, -2.8312e-03,\n",
       "                       -3.8728e-03, -4.4741e-03, -1.5866e-03, -3.5426e-03, -5.0190e-03,\n",
       "                       -9.7408e-03, -1.5916e-03,  1.2367e-02, -4.2040e-03, -5.3805e-03,\n",
       "                        3.5699e-03, -1.6477e-03, -2.7940e-03, -6.3602e-04, -1.3629e-02,\n",
       "                       -1.1041e-02,  2.4308e-03, -2.7937e-03,  4.0899e-03, -8.2523e-03,\n",
       "                       -1.2111e-03,  2.5770e-03, -6.5585e-03, -5.2251e-03, -7.9581e-03,\n",
       "                       -1.4953e-02,  3.2493e-03, -1.0784e-02, -1.6777e-03,  9.6705e-04,\n",
       "                       -9.8166e-03,  1.0510e-02,  4.3435e-03, -5.0098e-03,  2.6434e-03,\n",
       "                        5.6198e-03,  5.1633e-03, -2.5013e-03,  1.7498e-02, -1.6532e-03,\n",
       "                       -7.7581e-04, -2.3453e-04,  3.8980e-03, -1.3027e-02, -4.9866e-03,\n",
       "                       -7.7137e-03,  3.4825e-03, -9.8344e-03, -1.3812e-02,  1.4519e-03,\n",
       "                        3.0209e-03, -1.1840e-03,  1.0575e-03, -1.7581e-02, -1.5200e-03,\n",
       "                       -4.2940e-03,  1.9562e-03, -5.3398e-03, -1.0116e-02,  8.0080e-03,\n",
       "                       -2.0009e-03, -8.9207e-03,  2.9640e-03, -6.2578e-03, -3.0338e-03,\n",
       "                       -1.4281e-03,  3.4110e-04, -3.9704e-03, -8.4953e-03, -5.8585e-03,\n",
       "                       -2.4037e-03, -5.5077e-03, -5.2193e-03,  2.9861e-03, -3.2737e-03,\n",
       "                       -2.6426e-03, -5.5197e-04,  2.5667e-03, -3.0428e-03, -2.7705e-03,\n",
       "                        7.8185e-04,  1.3068e-03,  3.1033e-03,  3.8762e-03,  2.7949e-03,\n",
       "                       -3.2634e-04,  3.2841e-03, -2.6748e-03, -1.2640e-02,  1.7023e-03,\n",
       "                       -3.0639e-03, -7.3903e-03,  3.1160e-03, -6.5947e-04,  5.6995e-03,\n",
       "                       -2.5878e-03,  1.1308e-05,  3.4860e-03,  1.8862e-03, -3.7123e-03,\n",
       "                       -2.3559e-03, -1.0868e-02, -5.6489e-03, -2.1582e-03, -8.3944e-04,\n",
       "                       -5.7235e-03,  2.1022e-03, -7.7756e-04,  3.1551e-03,  3.1793e-03,\n",
       "                       -5.3054e-03, -2.6890e-03, -7.6467e-03, -1.2972e-02, -3.7586e-03,\n",
       "                       -3.2570e-03, -3.8554e-03, -1.1995e-02,  3.7137e-03, -3.0480e-03,\n",
       "                       -3.8545e-03,  3.6837e-03, -9.6851e-04,  7.9590e-03, -2.3000e-03,\n",
       "                        6.6563e-03,  5.3030e-03, -7.3107e-03, -6.2193e-03,  2.7147e-03,\n",
       "                        3.3246e-03, -3.1633e-03, -2.3035e-03,  1.2173e-02,  2.6210e-03,\n",
       "                       -8.3981e-03, -3.4800e-03,  2.7214e-03, -1.2055e-03, -1.2864e-03,\n",
       "                        1.6568e-03, -1.0202e-02, -1.4134e-02, -1.6661e-03,  3.3142e-03,\n",
       "                       -7.0975e-04, -5.7127e-03, -3.3600e-03, -7.5816e-03, -4.4213e-03,\n",
       "                       -5.8110e-03, -4.2595e-03,  8.6277e-04,  1.6834e-03, -4.5674e-03,\n",
       "                       -1.2477e-02,  2.1003e-04, -7.6269e-03, -6.3443e-03, -4.3115e-03,\n",
       "                       -3.3863e-03,  1.1297e-03, -5.2551e-03, -6.3181e-03,  2.7286e-03,\n",
       "                       -4.5893e-03, -6.1105e-04, -1.6488e-03, -6.5900e-03, -3.5579e-03,\n",
       "                        5.8407e-04,  4.7956e-03,  4.1670e-03,  4.2938e-03,  1.1212e-02,\n",
       "                       -1.5060e-03, -6.5195e-03,  2.6369e-03,  2.8048e-03,  6.1370e-04,\n",
       "                       -1.8866e-03,  3.0889e-03, -6.2715e-04, -2.9162e-03,  6.7447e-03,\n",
       "                        3.3872e-03,  3.5548e-03,  1.1661e-03,  6.8013e-03, -6.0130e-03,\n",
       "                        2.7081e-03, -3.7063e-03,  9.5782e-04, -9.7655e-03,  5.2270e-03,\n",
       "                        1.2908e-03, -1.7928e-03, -3.9196e-03,  3.4483e-03, -6.7359e-03,\n",
       "                       -4.5572e-03,  8.9210e-03,  1.0140e-02,  1.4164e-03, -7.3421e-03,\n",
       "                       -7.9985e-04])),\n",
       "              ('bn1.running_mean',\n",
       "               tensor([-2.1594e-02,  1.3415e-02, -7.0981e-03, -6.5816e-03,  7.6487e-03,\n",
       "                       -1.4922e-02,  3.2850e-03, -6.6761e-03,  4.3146e-04, -9.1664e-03,\n",
       "                        2.0835e-03,  1.2419e-02,  1.2975e-02, -1.0227e-02,  3.9561e-03,\n",
       "                       -2.6044e-03, -1.7694e-02,  1.6853e-03,  1.1904e-02, -4.3791e-03,\n",
       "                       -4.3080e-03, -1.6084e-02, -9.4125e-04, -6.7437e-03, -8.9478e-03,\n",
       "                       -1.0222e-02, -1.0041e-02, -2.8778e-02,  1.2817e-03, -6.3228e-03,\n",
       "                        5.9394e-04,  4.3081e-03, -8.3678e-04, -5.0141e-03,  2.6314e-03,\n",
       "                       -3.5501e-03, -5.9222e-03,  1.0210e-02,  3.8378e-03,  3.7326e-03,\n",
       "                       -2.1817e-02,  4.8955e-03, -2.8108e-03, -1.4845e-02,  6.9024e-03,\n",
       "                       -7.5588e-03, -6.2821e-03,  8.5571e-03, -6.8959e-04, -9.7076e-03,\n",
       "                       -3.8966e-03,  5.9188e-03,  4.3107e-03,  9.3872e-03,  1.1836e-02,\n",
       "                        3.1395e-03,  1.0041e-02,  3.6273e-04, -4.7412e-03, -4.6599e-03,\n",
       "                        8.4657e-03,  1.9381e-02, -3.6222e-03,  9.5652e-03,  2.0402e-03,\n",
       "                       -1.6396e-03, -1.8090e-02,  2.7448e-03,  1.4793e-03, -7.4205e-03,\n",
       "                        8.2202e-03,  1.5823e-02,  7.0294e-04, -6.2648e-03,  8.8588e-03,\n",
       "                        5.8507e-05, -5.1916e-04,  3.7608e-03,  1.7493e-03, -2.6282e-03,\n",
       "                        1.0472e-02, -1.7125e-03, -3.3807e-03,  9.7001e-03, -1.0187e-03,\n",
       "                       -3.6739e-03, -4.9091e-03,  4.6819e-04, -8.9153e-03, -8.4928e-03,\n",
       "                        6.4286e-03, -6.4042e-03,  6.9500e-03, -1.5745e-04, -1.4820e-03,\n",
       "                       -1.3441e-02, -7.2610e-03,  2.2850e-03, -4.9389e-03,  1.6268e-02,\n",
       "                        3.3668e-03, -6.3513e-03,  4.3604e-03,  1.0373e-02, -7.8878e-03,\n",
       "                       -4.4091e-03, -4.1024e-03,  1.0318e-02,  5.9965e-03,  7.8576e-03,\n",
       "                       -2.0512e-02,  7.7247e-03, -5.0755e-04,  1.6765e-03, -7.1031e-03,\n",
       "                       -5.6090e-03, -2.5753e-03,  2.0358e-03,  8.3826e-03, -5.4372e-03,\n",
       "                        5.9130e-03, -1.5262e-03,  4.1764e-03, -5.5724e-03, -6.5525e-03,\n",
       "                       -7.4570e-03,  4.4982e-03,  1.2617e-02,  2.0968e-04, -1.5943e-03,\n",
       "                       -4.4053e-03, -9.0076e-03, -2.0146e-03,  3.5051e-03,  1.7371e-03,\n",
       "                       -3.5194e-03, -3.7891e-03,  2.5079e-03, -3.4169e-03, -1.2003e-02,\n",
       "                        4.3402e-04, -9.4733e-04,  1.4804e-02, -7.8698e-03, -7.9453e-03,\n",
       "                        5.9784e-03, -2.6949e-03,  1.3584e-03, -9.0980e-03, -7.1156e-04,\n",
       "                        3.2532e-07, -1.1540e-02, -1.0418e-03, -3.9377e-03, -1.1556e-02,\n",
       "                       -1.0045e-02, -5.2787e-03,  8.8883e-03, -5.4580e-04,  1.5000e-03,\n",
       "                       -3.0944e-03,  6.8952e-03,  4.2793e-03,  2.0329e-02, -2.2440e-04,\n",
       "                       -4.6968e-03, -2.2868e-02,  1.6640e-03, -2.4028e-02, -1.4383e-03,\n",
       "                        2.9824e-03,  8.5209e-03,  8.5600e-03,  9.6886e-03, -5.3643e-03,\n",
       "                       -8.4390e-03, -1.8740e-03,  1.2891e-03, -4.4531e-03, -1.2025e-02,\n",
       "                        7.9309e-03,  1.1149e-02,  1.3561e-02, -1.2652e-02, -5.4444e-03,\n",
       "                        8.2408e-03,  1.6657e-03, -1.6935e-02,  6.3210e-03,  3.1707e-03,\n",
       "                        6.8747e-03, -2.7092e-04,  1.7121e-02,  1.2465e-02, -4.5953e-03,\n",
       "                        1.3877e-03, -1.4890e-02, -1.9201e-02, -5.1014e-03, -4.8693e-03,\n",
       "                       -1.6931e-02,  1.3917e-02,  1.0500e-02, -6.2360e-03, -7.9692e-03,\n",
       "                        5.2032e-03,  7.3157e-03,  1.6834e-02,  1.5104e-03, -1.2608e-02,\n",
       "                       -9.4564e-03, -1.8950e-02, -1.6722e-03,  1.5412e-03,  1.8944e-02,\n",
       "                        2.7432e-03, -1.2966e-02,  1.5154e-03, -3.7717e-03, -1.7280e-02,\n",
       "                       -7.1903e-03, -7.1850e-03, -5.4798e-03,  1.4327e-03,  1.6332e-04,\n",
       "                       -2.6838e-03,  1.0781e-02,  7.0018e-03,  9.1194e-03, -3.5157e-03,\n",
       "                        1.1594e-02, -8.1871e-03,  5.2176e-03, -1.5148e-02, -3.8085e-03,\n",
       "                       -1.2149e-03,  4.2972e-03, -6.4559e-03, -7.3158e-03, -1.9504e-03,\n",
       "                        2.0854e-03, -4.9709e-03, -7.9189e-04, -7.0234e-03,  5.1366e-03,\n",
       "                       -2.7697e-03, -8.4295e-03, -2.1549e-03, -1.1507e-02,  1.5929e-02,\n",
       "                       -3.7719e-03,  2.5336e-03, -4.9520e-03,  1.0382e-02,  1.2763e-02,\n",
       "                       -1.0835e-02])),\n",
       "              ('bn1.running_var',\n",
       "               tensor([0.2749, 0.0950, 0.2071, 0.3468, 0.1716, 0.6184, 0.1290, 0.1862, 0.1102,\n",
       "                       0.1236, 0.1694, 0.3422, 0.2095, 0.2222, 0.1913, 0.1222, 0.4410, 0.2157,\n",
       "                       0.6560, 0.0925, 0.2239, 0.6376, 0.1233, 0.2453, 0.1604, 0.4140, 0.4323,\n",
       "                       0.7638, 0.1080, 0.4722, 0.1124, 0.1835, 0.1213, 0.0835, 0.1040, 0.1179,\n",
       "                       0.2160, 0.1264, 0.1750, 0.3186, 0.4819, 0.1895, 0.1324, 0.1370, 0.1406,\n",
       "                       0.8308, 0.2655, 0.1825, 0.1067, 0.7908, 0.0603, 0.2069, 0.2398, 0.4371,\n",
       "                       0.5657, 0.4924, 0.4560, 0.0745, 0.1214, 0.0853, 0.1122, 0.1462, 0.1030,\n",
       "                       0.6905, 0.1829, 0.2043, 0.9660, 0.1061, 0.1354, 0.1266, 0.2618, 0.5320,\n",
       "                       0.1212, 0.3235, 0.1671, 0.0951, 0.1743, 0.1288, 0.1866, 0.2634, 0.3388,\n",
       "                       0.1919, 0.0975, 0.2935, 0.2011, 0.1023, 0.1651, 0.1407, 0.0561, 0.2445,\n",
       "                       0.2234, 0.3276, 0.3145, 0.0960, 0.1853, 0.7239, 0.4336, 0.1134, 0.2255,\n",
       "                       0.2753, 0.2592, 0.2893, 0.0925, 0.2198, 0.2935, 0.1584, 0.2493, 0.3739,\n",
       "                       0.2733, 0.2168, 0.3278, 0.5969, 0.4472, 0.0875, 0.1913, 0.2876, 0.5721,\n",
       "                       0.1143, 0.5644, 0.1578, 0.3679, 0.1032, 0.1114, 0.1422, 0.1476, 0.2014,\n",
       "                       0.1526, 0.8750, 0.0841, 0.1441, 0.1172, 0.2796, 0.1498, 0.1330, 0.1031,\n",
       "                       0.1502, 0.1617, 0.1178, 0.1514, 0.0925, 0.5216, 0.1510, 0.7886, 0.4177,\n",
       "                       0.5343, 0.3118, 0.1464, 0.1513, 0.1835, 0.1012, 0.4548, 0.1588, 0.3349,\n",
       "                       0.1721, 0.4537, 0.3042, 0.1316, 0.3751, 0.1776, 0.1070, 0.1371, 0.1594,\n",
       "                       0.1482, 0.0871, 0.1178, 0.1590, 1.4802, 0.0931, 0.5112, 0.1510, 0.0582,\n",
       "                       0.3119, 0.1661, 0.4805, 0.1819, 0.3420, 0.3068, 0.2693, 0.0993, 0.3141,\n",
       "                       0.2003, 0.4712, 0.4527, 0.5981, 0.2054, 0.0721, 0.1745, 0.3711, 0.2005,\n",
       "                       0.1879, 0.7052, 0.0560, 0.3643, 0.4605, 0.1578, 0.4473, 0.6315, 0.4314,\n",
       "                       0.1130, 0.3085, 0.2278, 0.7610, 0.0513, 0.3584, 0.1381, 0.1923, 0.1592,\n",
       "                       0.2100, 0.1901, 0.3579, 0.2480, 0.3091, 0.1806, 0.0977, 0.5155, 0.1437,\n",
       "                       0.4107, 0.1901, 0.0720, 0.1138, 0.1476, 0.2126, 0.2070, 0.0932, 0.0712,\n",
       "                       0.2686, 0.6121, 0.2418, 0.4788, 0.2284, 0.1104, 0.2354, 0.1889, 0.2383,\n",
       "                       0.0965, 0.1251, 0.1486, 0.1906, 0.3958, 0.1808, 0.1075, 0.2691, 0.2431,\n",
       "                       0.3091, 0.3117, 0.1656, 0.1850, 0.1151, 0.0848, 0.3664, 0.2047, 0.1456,\n",
       "                       0.4751, 0.8178, 0.6216, 0.0781])),\n",
       "              ('bn1.num_batches_tracked', tensor(50)),\n",
       "              ('fc2.weight',\n",
       "               tensor([[-0.0280, -0.0068, -0.0144,  ...,  0.0260,  0.0095,  0.0025],\n",
       "                       [ 0.0035,  0.0031,  0.0041,  ...,  0.0080, -0.0223,  0.0061],\n",
       "                       [-0.0187,  0.0083, -0.0080,  ..., -0.0132, -0.0212, -0.0015],\n",
       "                       ...,\n",
       "                       [-0.0307, -0.0176, -0.0030,  ...,  0.0074, -0.0124,  0.0038],\n",
       "                       [-0.0424,  0.0383,  0.0386,  ...,  0.0067,  0.0158,  0.0089],\n",
       "                       [ 0.0204,  0.0157,  0.0063,  ...,  0.0032,  0.0153, -0.0021]])),\n",
       "              ('fc2.bias',\n",
       "               tensor([ 1.7152e-02,  4.6084e-04,  3.4592e-05,  1.7586e-03, -3.8446e-04,\n",
       "                       -1.3906e-03, -3.4947e-05, -1.9648e-04,  3.4545e-04, -1.3670e-03,\n",
       "                        1.1731e-04,  7.1764e-03,  7.3083e-03,  3.6088e-03, -1.3506e-03,\n",
       "                       -7.9734e-04, -7.1842e-03,  4.6363e-05,  2.2387e-03,  2.2057e-04,\n",
       "                        2.6958e-03, -2.0554e-04, -7.1295e-05, -1.2202e-04, -7.8206e-04,\n",
       "                       -4.7574e-04, -1.0764e-04,  4.9207e-04,  7.0023e-05,  7.5520e-03,\n",
       "                       -4.9757e-04, -1.5285e-02,  1.6416e-04,  4.0353e-03,  1.9212e-03,\n",
       "                       -8.0890e-04, -1.1921e-02,  1.0101e-03,  6.0788e-03,  5.8386e-05,\n",
       "                       -8.3514e-03, -1.3778e-03,  2.4649e-04,  1.4353e-03, -1.6908e-02,\n",
       "                        7.2231e-04, -1.0075e-03, -4.9102e-03,  2.6723e-03, -7.5418e-04,\n",
       "                        7.8581e-04,  8.5324e-04,  1.0558e-03, -1.9110e-02,  8.9437e-03,\n",
       "                        1.2892e-04, -1.6002e-02, -6.4189e-04, -1.7871e-02, -9.0330e-04,\n",
       "                        1.0811e-02,  1.6605e-04, -2.8275e-04, -1.4902e-04, -8.6917e-04,\n",
       "                       -8.8655e-03,  6.7565e-03, -4.3568e-03,  8.2501e-04, -3.6121e-04,\n",
       "                        1.8649e-03,  2.7749e-04, -1.5226e-02,  8.0708e-04,  1.7862e-02,\n",
       "                        8.5799e-04,  5.6560e-05,  1.6811e-02,  7.1612e-05, -3.8387e-04,\n",
       "                        6.7911e-04,  6.4853e-04, -6.5690e-03,  4.2599e-04, -1.9589e-04,\n",
       "                        1.9130e-03, -4.2911e-05,  1.3927e-03, -1.7896e-02, -2.8602e-04,\n",
       "                       -4.5630e-04,  1.1139e-04,  6.4752e-04, -1.8756e-03,  1.5091e-06,\n",
       "                        6.3494e-04,  9.4864e-03, -1.9073e-03, -1.7945e-03,  4.6679e-04,\n",
       "                       -1.8984e-04, -4.4226e-04,  3.0006e-04, -1.3355e-02, -6.1842e-04,\n",
       "                       -2.0434e-04, -5.2237e-03, -2.7854e-04, -2.0025e-03,  1.8687e-02,\n",
       "                       -8.3838e-04, -6.9921e-06,  1.9740e-03, -7.9265e-04,  1.1143e-02,\n",
       "                        9.7381e-03,  2.4426e-03,  1.8629e-02, -1.9681e-03,  4.1790e-04,\n",
       "                       -1.2016e-03, -1.6362e-02,  7.6503e-05, -1.9981e-03,  2.2246e-03,\n",
       "                        9.0679e-04,  7.8464e-04, -7.6418e-04])),\n",
       "              ('bn2.weight',\n",
       "               tensor([0.9506, 0.9505, 0.9504, 0.9508, 0.9505, 0.9498, 0.9502, 0.9502, 0.9511,\n",
       "                       0.9504, 0.9499, 0.9505, 0.9503, 0.9507, 0.9506, 0.9524, 0.9506, 0.9503,\n",
       "                       0.9507, 0.9502, 0.9506, 0.9508, 0.9503, 0.9505, 0.9505, 0.9500, 0.9509,\n",
       "                       0.9499, 0.9504, 0.9507, 0.9510, 0.9505, 0.9500, 0.9508, 0.9498, 0.9509,\n",
       "                       0.9498, 0.9499, 0.9506, 0.9499, 0.9505, 0.9507, 0.9525, 0.9503, 0.9499,\n",
       "                       0.9492, 0.9507, 0.9492, 0.9505, 0.9496, 0.9496, 0.9516, 0.9505, 0.9500,\n",
       "                       0.9500, 0.9506, 0.9501, 0.9509, 0.9507, 0.9500, 0.9508, 0.9505, 0.9498,\n",
       "                       0.9500, 0.9501, 0.9503, 0.9503, 0.9503, 0.9503, 0.9502, 0.9501, 0.9511,\n",
       "                       0.9508, 0.9511, 0.9498, 0.9509, 0.9511, 0.9508, 0.9499, 0.9509, 0.9505,\n",
       "                       0.9508, 0.9500, 0.9506, 0.9500, 0.9503, 0.9501, 0.9504, 0.9506, 0.9504,\n",
       "                       0.9509, 0.9504, 0.9508, 0.9502, 0.9504, 0.9498, 0.9499, 0.9499, 0.9496,\n",
       "                       0.9505, 0.9505, 0.9506, 0.9503, 0.9503, 0.9501, 0.9502, 0.9504, 0.9503,\n",
       "                       0.9500, 0.9507, 0.9503, 0.9501, 0.9506, 0.9506, 0.9505, 0.9504, 0.9503,\n",
       "                       0.9502, 0.9504, 0.9504, 0.9505, 0.9497, 0.9509, 0.9503, 0.9504, 0.9494,\n",
       "                       0.9505, 0.9506])),\n",
       "              ('bn2.bias',\n",
       "               tensor([-6.5357e-03, -1.4483e-03,  7.5619e-03,  7.4529e-04,  3.5032e-03,\n",
       "                       -5.8510e-03, -1.1383e-02,  3.7123e-03, -1.2276e-02,  7.4581e-03,\n",
       "                       -7.4123e-04,  7.6714e-04, -2.0395e-03, -5.3975e-03,  2.4206e-05,\n",
       "                       -7.2126e-03,  2.1336e-03,  6.3681e-03,  4.1271e-03,  1.0441e-02,\n",
       "                        1.3912e-03, -4.0428e-03,  1.4006e-03, -2.6252e-03, -7.3890e-03,\n",
       "                        5.1850e-03, -1.1038e-03,  2.6296e-03, -8.1255e-03, -9.4624e-03,\n",
       "                        4.4977e-03,  5.4934e-03,  4.9610e-03, -1.1169e-02,  9.7235e-03,\n",
       "                        1.0179e-02,  6.8244e-03,  1.4014e-02,  2.7042e-03,  7.2381e-03,\n",
       "                       -3.9726e-04,  8.0669e-03,  3.7457e-03, -2.5047e-04,  1.0488e-02,\n",
       "                        1.8198e-02, -3.9798e-03,  1.3437e-02,  1.1087e-02,  8.4188e-03,\n",
       "                        1.2740e-02, -6.8275e-03, -1.2372e-02, -4.0298e-03,  1.2823e-02,\n",
       "                       -5.8546e-03, -6.6085e-04, -5.6805e-03, -1.8388e-02, -4.1226e-03,\n",
       "                       -4.6766e-03, -3.9681e-03, -7.9025e-04,  6.6858e-03, -9.4621e-03,\n",
       "                       -1.8317e-02,  4.5065e-03,  9.9280e-03, -1.1825e-02, -5.8570e-03,\n",
       "                        8.6453e-03,  9.5456e-03, -7.1864e-03, -1.4078e-03, -6.4756e-03,\n",
       "                       -3.2971e-03, -8.4598e-03, -4.9860e-04,  1.7685e-03, -1.6947e-03,\n",
       "                       -8.8253e-03,  1.1369e-02, -6.5436e-03, -3.7222e-03,  1.0621e-02,\n",
       "                        4.8666e-03,  3.8562e-03, -5.2297e-03,  4.0328e-03, -6.6419e-03,\n",
       "                       -3.9964e-03, -3.4803e-04,  9.1188e-04, -1.0789e-02,  1.2847e-02,\n",
       "                        5.5523e-03,  4.3616e-03, -8.1685e-03,  1.4350e-02, -7.2460e-03,\n",
       "                       -1.1379e-02, -7.3144e-03,  2.7289e-04, -1.5535e-02,  1.2777e-02,\n",
       "                       -2.0844e-03, -4.2269e-03, -4.6245e-03,  2.2047e-03, -1.7079e-02,\n",
       "                       -3.9766e-03, -1.1158e-02, -7.8953e-03, -9.2293e-03, -1.8341e-02,\n",
       "                       -7.3880e-03, -1.0695e-02, -3.0061e-03, -3.8423e-03, -1.4468e-03,\n",
       "                       -1.0311e-02,  7.9852e-03, -3.4341e-03, -4.5381e-03,  4.5293e-03,\n",
       "                       -4.6625e-03, -9.7133e-04, -9.0768e-03])),\n",
       "              ('bn2.running_mean',\n",
       "               tensor([-0.1113, -0.0808, -0.1142,  0.0584, -0.1937, -0.1276, -0.2137, -0.0379,\n",
       "                        0.2895, -0.0812,  0.3124,  0.0631, -0.0973, -0.1719,  0.1023, -0.1982,\n",
       "                       -0.2937,  0.0555,  0.0548,  0.0174, -0.2289,  0.1134,  0.2162, -0.0353,\n",
       "                        0.1454, -0.0193, -0.0206,  0.0230, -0.0354,  0.1600, -0.0112,  0.0757,\n",
       "                       -0.1858, -0.1517, -0.0569,  0.0276, -0.0032,  0.0868, -0.0255,  0.1215,\n",
       "                       -0.1286, -0.1240, -0.1923,  0.1136, -0.1170,  0.2734, -0.0200,  0.3701,\n",
       "                       -0.0777,  0.0669, -0.1024, -0.1735, -0.2602, -0.2958, -0.1243, -0.2803,\n",
       "                       -0.1546,  0.0824, -0.2955,  0.0805, -0.0519, -0.1123,  0.0580,  0.0276,\n",
       "                        0.1786, -0.1616,  0.2127, -0.1983, -0.1922, -0.2372, -0.1076, -0.1245,\n",
       "                       -0.1122,  0.0701, -0.3189,  0.0687,  0.1941,  0.1356, -0.0694, -0.0469,\n",
       "                       -0.1514,  0.0207, -0.0184,  0.1618,  0.2360, -0.1276,  0.1053,  0.2193,\n",
       "                        0.1235, -0.1317, -0.1499,  0.0481, -0.1160,  0.2050, -0.1088,  0.1882,\n",
       "                       -0.1190,  0.2381,  0.1677, -0.1370, -0.0738, -0.0887,  0.1148,  0.2440,\n",
       "                        0.1513,  0.2069,  0.0859, -0.1035,  0.3806, -0.2431,  0.0061, -0.1249,\n",
       "                        0.0705,  0.1275, -0.2599,  0.2097,  0.4144,  0.2698, -0.1576,  0.0030,\n",
       "                       -0.0882, -0.0254, -0.0232, -0.0925,  0.1096,  0.3537,  0.0524, -0.2643])),\n",
       "              ('bn2.running_var',\n",
       "               tensor([0.1210, 0.2014, 0.1198, 0.2880, 0.2122, 0.1778, 0.1089, 0.0836, 0.2315,\n",
       "                       0.1579, 0.2005, 0.0820, 0.0611, 0.2515, 0.1204, 0.1385, 0.1596, 0.1640,\n",
       "                       0.2716, 0.0858, 0.3087, 0.2103, 0.3246, 0.2018, 0.1650, 0.1636, 0.0884,\n",
       "                       0.1280, 0.0591, 0.1226, 0.0828, 0.0634, 0.4586, 0.0724, 0.1320, 0.0978,\n",
       "                       0.1346, 0.2403, 0.1372, 0.1067, 0.0884, 0.0594, 0.1234, 0.1822, 0.2208,\n",
       "                       0.3082, 0.1016, 0.3862, 0.1614, 0.1112, 0.1700, 0.1604, 0.1620, 0.2980,\n",
       "                       0.1692, 0.1651, 0.0697, 0.1159, 0.1626, 0.1496, 0.1007, 0.0920, 0.1545,\n",
       "                       0.1080, 0.1175, 0.1771, 0.3449, 0.3315, 0.0823, 0.0921, 0.0946, 0.1089,\n",
       "                       0.2406, 0.2241, 0.1281, 0.1003, 0.1565, 0.1511, 0.0898, 0.0728, 0.1604,\n",
       "                       0.1217, 0.1212, 0.3480, 0.2198, 0.2050, 0.1480, 0.4443, 0.1778, 0.1754,\n",
       "                       0.1456, 0.1111, 0.0491, 0.1176, 0.2315, 0.1651, 0.0739, 0.4443, 0.1530,\n",
       "                       0.1939, 0.0880, 0.0754, 0.1494, 0.1005, 0.1343, 0.1782, 0.2956, 0.1587,\n",
       "                       0.2503, 0.1348, 0.1311, 0.0954, 0.0875, 0.2474, 0.1138, 0.1315, 0.3678,\n",
       "                       0.6418, 0.1024, 0.0791, 0.0826, 0.1110, 0.1107, 0.1174, 0.1072, 0.2855,\n",
       "                       0.0936, 0.3525])),\n",
       "              ('bn2.num_batches_tracked', tensor(50)),\n",
       "              ('fc3.weight',\n",
       "               tensor([[-0.0102,  0.0011,  0.0227,  ..., -0.0314, -0.0111,  0.0431],\n",
       "                       [-0.0076,  0.0031, -0.0434,  ..., -0.0184, -0.0271,  0.0310],\n",
       "                       [ 0.0365,  0.0279, -0.0393,  ...,  0.0569,  0.0540,  0.0157],\n",
       "                       ...,\n",
       "                       [-0.0150,  0.0127, -0.0414,  ..., -0.0487, -0.0064,  0.0407],\n",
       "                       [-0.0255,  0.0133,  0.0100,  ...,  0.0539,  0.0306,  0.0049],\n",
       "                       [-0.0335,  0.0290,  0.0421,  ..., -0.0302, -0.0410, -0.0467]])),\n",
       "              ('fc3.bias',\n",
       "               tensor([-3.3103e-03, -8.4664e-04, -3.7030e-02, -3.1745e-02,  7.0685e-04,\n",
       "                       -7.0380e-04,  1.0076e-03, -1.9719e-02,  8.2486e-03, -8.0313e-03,\n",
       "                        4.0928e-02, -2.0332e-02, -5.7867e-05, -3.0025e-03, -2.2563e-02,\n",
       "                       -6.4275e-04,  1.0126e-02, -3.7447e-02,  3.9799e-02, -3.3076e-04,\n",
       "                        4.7610e-04, -1.1038e-02,  7.2499e-04,  6.7305e-03, -8.4581e-04,\n",
       "                       -4.4430e-04,  1.2093e-03, -1.2014e-03,  6.0570e-03, -2.0850e-02,\n",
       "                        4.5162e-04,  8.2800e-04,  1.7246e-03, -2.7352e-02, -2.6172e-02,\n",
       "                       -5.3454e-03, -3.4822e-04,  8.6601e-04,  6.3097e-05,  3.0430e-04,\n",
       "                       -1.9505e-02,  1.6561e-02, -1.0444e-02,  6.1570e-03, -1.0953e-02,\n",
       "                       -1.9516e-03, -4.0932e-02, -6.0794e-03,  1.4607e-03, -2.1616e-02,\n",
       "                       -5.2171e-04,  3.6180e-02, -3.3290e-02, -3.2928e-03,  5.4083e-03,\n",
       "                       -3.2821e-02,  3.3148e-02, -1.2129e-02, -1.8905e-03,  3.3012e-03,\n",
       "                       -2.0688e-02,  1.5250e-02,  2.7430e-02, -7.9859e-05])),\n",
       "              ('bn3.weight',\n",
       "               tensor([0.9508, 0.9525, 0.9553, 0.9551, 0.9509, 0.9514, 0.9512, 0.9632, 0.9509,\n",
       "                       0.9543, 0.9501, 0.9510, 0.9498, 0.9528, 0.9509, 0.9489, 0.9509, 0.9525,\n",
       "                       0.9525, 0.9541, 0.9514, 0.9504, 0.9505, 0.9495, 0.9517, 0.9499, 0.9534,\n",
       "                       0.9502, 0.9519, 0.9523, 0.9494, 0.9496, 0.9504, 0.9495, 0.9535, 0.9551,\n",
       "                       0.9505, 0.9520, 0.9632, 0.9496, 0.9524, 0.9508, 0.9685, 0.9494, 0.9490,\n",
       "                       0.9505, 0.9569, 0.9525, 0.9778, 0.9533, 0.9511, 0.9503, 0.9556, 0.9496,\n",
       "                       0.9627, 0.9541, 0.9506, 0.9540, 0.9541, 0.9505, 0.9518, 0.9502, 0.9533,\n",
       "                       0.9532])),\n",
       "              ('bn3.bias',\n",
       "               tensor([-0.0007,  0.0122, -0.0163, -0.0162, -0.0040, -0.0101, -0.0106,  0.0115,\n",
       "                       -0.0096, -0.0151,  0.0043, -0.0117,  0.0126, -0.0143, -0.0087,  0.0150,\n",
       "                       -0.0073, -0.0146, -0.0134, -0.0156, -0.0143,  0.0080,  0.0023,  0.0109,\n",
       "                        0.0117,  0.0096, -0.0144,  0.0150, -0.0159, -0.0134,  0.0099,  0.0129,\n",
       "                        0.0025,  0.0135, -0.0160, -0.0152,  0.0088, -0.0119,  0.0103,  0.0118,\n",
       "                        0.0131, -0.0017,  0.0116,  0.0129,  0.0129,  0.0110, -0.0160,  0.0170,\n",
       "                        0.0108, -0.0147, -0.0110,  0.0123, -0.0161,  0.0157,  0.0121,  0.0102,\n",
       "                        0.0146, -0.0151, -0.0143,  0.0087, -0.0132,  0.0120, -0.0148, -0.0148])),\n",
       "              ('bn3.running_mean',\n",
       "               tensor([ 0.0123, -0.1487, -0.0713, -0.1071,  0.0312,  0.1429,  0.1517, -0.2479,\n",
       "                        0.1346, -0.0243, -0.0624,  0.0653, -0.1000,  0.2627,  0.0671, -0.2376,\n",
       "                       -0.0852, -0.0050, -0.0341, -0.0413,  0.2240, -0.0753, -0.1083, -0.1487,\n",
       "                       -0.1486, -0.1553, -0.0393, -0.0664,  0.1934, -0.1189, -0.1134, -0.2727,\n",
       "                       -0.0363, -0.0884,  0.1993, -0.0791, -0.1530, -0.0473, -0.1759, -0.1649,\n",
       "                       -0.1558,  0.0886, -0.2686, -0.1401, -0.0177, -0.2843, -0.0581, -0.0940,\n",
       "                       -0.2175, -0.0030,  0.0941, -0.0583,  0.0776, -0.3000, -0.2171, -0.1138,\n",
       "                       -0.1773, -0.1109, -0.0443,  0.2390,  0.1841,  0.0044,  0.1373, -0.1120])),\n",
       "              ('bn3.running_var',\n",
       "               tensor([0.0749, 0.5736, 0.2493, 0.2375, 0.1191, 0.0678, 0.1089, 0.3267, 0.0711,\n",
       "                       0.0862, 0.1261, 0.1223, 0.2005, 0.4296, 0.0881, 0.1302, 0.1233, 0.1083,\n",
       "                       0.1482, 0.1557, 0.2452, 0.0978, 0.0865, 0.2179, 0.2839, 0.1328, 0.1149,\n",
       "                       0.1334, 0.2699, 0.0664, 0.2118, 0.3550, 0.0809, 0.2241, 0.6058, 0.0637,\n",
       "                       0.0581, 0.0818, 0.2993, 0.1646, 0.5022, 0.0528, 0.4206, 0.1301, 0.1638,\n",
       "                       0.2445, 0.4112, 0.3468, 0.4923, 0.0662, 0.0777, 0.6820, 0.1696, 0.1962,\n",
       "                       0.4396, 0.2364, 0.0977, 0.1145, 0.1468, 0.1860, 0.1456, 0.2143, 0.0659,\n",
       "                       0.1666])),\n",
       "              ('bn3.num_batches_tracked', tensor(50)),\n",
       "              ('fc4.weight',\n",
       "               tensor([[ 0.0328,  0.0577, -0.0672,  ..., -0.0627, -0.0647, -0.0467],\n",
       "                       [ 0.0941, -0.0323, -0.0695,  ...,  0.0459, -0.0618, -0.0906],\n",
       "                       [-0.0959,  0.0397, -0.0738,  ...,  0.0095, -0.0775,  0.0614],\n",
       "                       ...,\n",
       "                       [ 0.0512, -0.0691,  0.0105,  ...,  0.0808,  0.0102, -0.0351],\n",
       "                       [ 0.0435, -0.0167, -0.1159,  ..., -0.0594, -0.0011,  0.0985],\n",
       "                       [ 0.0232, -0.1072,  0.0745,  ...,  0.0771,  0.1003,  0.0604]])),\n",
       "              ('fc4.bias',\n",
       "               tensor([ 4.8166e-02,  1.8785e-02, -1.3247e-01, -7.7263e-02, -8.8978e-02,\n",
       "                       -3.3837e-02,  9.9820e-02, -3.6096e-02,  1.0715e-01, -1.0759e-01,\n",
       "                       -5.1292e-02,  5.8210e-02, -3.7101e-02,  7.3795e-02, -8.6872e-05,\n",
       "                        7.3883e-02,  6.0457e-02, -1.1035e-01,  6.5594e-02, -7.2132e-02,\n",
       "                       -3.9092e-02, -2.6049e-02,  8.0894e-02,  4.9027e-02, -6.1654e-02,\n",
       "                        3.7278e-02, -1.1551e-01, -5.6707e-02,  1.2405e-02,  5.1247e-02,\n",
       "                       -1.1816e-01,  6.6344e-02])),\n",
       "              ('fc5.weight',\n",
       "               tensor([[ 0.0967,  0.0364,  0.0693,  0.1104, -0.0830,  0.1586,  0.1464, -0.1038,\n",
       "                        -0.0673,  0.1098, -0.1301,  0.0379, -0.0095,  0.1123, -0.1735,  0.0848,\n",
       "                         0.0967,  0.1045,  0.0496, -0.1488, -0.1497,  0.0574, -0.0393, -0.1359,\n",
       "                         0.0048,  0.0845,  0.0238,  0.1723, -0.1232,  0.0207,  0.0937,  0.1482]])),\n",
       "              ('fc5.bias', tensor([-0.0802]))]),\n",
       " [0.5862938165664673, 0.5991207957267761, 0.5887470245361328])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Perform cross-validation\n",
    "cross_val_train(RegressionNN, X_train, y_train, epochs=50, k_folds=3, patience=50)\n",
    "# Save the model\n",
    "# Example usage: torch.save(model.state_dict(), \"enhanced_regression_model.pth\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Antonia\\AppData\\Local\\Temp\\ipykernel_9536\\2036549851.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pth\"))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for RegressionNN:\n\tMissing key(s) in state_dict: \"fc1.weight\", \"fc1.bias\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"fc2.weight\", \"fc2.bias\", \"bn2.weight\", \"bn2.bias\", \"bn2.running_mean\", \"bn2.running_var\", \"fc3.weight\", \"fc3.bias\", \"bn3.weight\", \"bn3.bias\", \"bn3.running_mean\", \"bn3.running_var\", \"fc4.weight\", \"fc4.bias\", \"fc5.weight\", \"fc5.bias\". \n\tUnexpected key(s) in state_dict: \"model_state\", \"fold_results\", \"mean_rmse\", \"std_rmse\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9536\\2036549851.py\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;31m# Train the best model on the full training set and evaluate on test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m \u001b[0mtrain_on_full_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRegressionNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9536\\2036549851.py\u001b[0m in \u001b[0;36mtrain_on_full_data\u001b[1;34m(model_class, X_train, y_train, X_test, y_test, criterion, epochs, patience)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mpatience_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"best_model.pth\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Antonia\\.conda\\envs\\MDMLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2214\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2215\u001b[1;33m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[0;32m   2216\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0;32m   2217\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for RegressionNN:\n\tMissing key(s) in state_dict: \"fc1.weight\", \"fc1.bias\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"fc2.weight\", \"fc2.bias\", \"bn2.weight\", \"bn2.bias\", \"bn2.running_mean\", \"bn2.running_var\", \"fc3.weight\", \"fc3.bias\", \"bn3.weight\", \"bn3.bias\", \"bn3.running_mean\", \"bn3.running_var\", \"fc4.weight\", \"fc4.bias\", \"fc5.weight\", \"fc5.bias\". \n\tUnexpected key(s) in state_dict: \"model_state\", \"fold_results\", \"mean_rmse\", \"std_rmse\". "
     ]
    }
   ],
   "source": [
    "# Load the best model and train it on the full training set\n",
    "def train_on_full_data(model_class, X_train, y_train, X_test, y_test, criterion, epochs=100,  patience=100):\n",
    "    model = model_class(X_train.shape[1])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=100, verbose=True)\n",
    "    # Load the best model state\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_test)\n",
    "            val_loss = criterion(val_outputs, y_test)\n",
    "\n",
    "\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "            break\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            rmse = torch.sqrt(loss).item()\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}], RMSE: {rmse:.4f}\")\n",
    "\n",
    "    print(\"Training on full dataset completed.\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test)\n",
    "        test_loss = criterion(test_outputs, y_test)\n",
    "        test_rmse = torch.sqrt(test_loss).item()\n",
    "        print(f\"\\nTest RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "    # Save the final model\n",
    "    torch.save(model.state_dict(), \"final_model.pth\")\n",
    "    print(\"Final model saved as 'final_model.pth'.\")\n",
    "\n",
    "# Train the best model on the full training set and evaluate on test set\n",
    "train_on_full_data(RegressionNN, X_train, y_train, X_test, y_test, criterion, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0.11333410441875458]\n",
    "[0.11041968315839767]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MDMLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
