{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fingerprint: Coulomb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Coulomb import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import KFold  \n",
    "import joblib  # For saving and loading scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=251)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Normalize the target (hform)\n",
    "#target_scaler = MinMaxScaler()  # You can use StandardScaler if needed\n",
    "#y_train = target_scaler.fit_transform(y_train.reshape(-1, 1) if isinstance(y_train, np.ndarray) else y_train.to_numpy().reshape(-1, 1))\n",
    "#y_test = target_scaler.transform(y_test.reshape(-1, 1) if isinstance(y_test, np.ndarray) else y_test.to_numpy().reshape(-1, 1))\n",
    "\n",
    "# Ensure y_train and y_test are properly converted to NumPy arrays\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "\n",
    "# Convert y_train and y_test to NumPy arrays if they are Series or other objects\n",
    "if isinstance(y_train, pd.Series):\n",
    "    y_train = y_train.to_numpy()\n",
    "\n",
    "if isinstance(y_test, pd.Series):\n",
    "    y_test = y_test.to_numpy()\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Add dimension\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)  # Add dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(RegressionNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)  # Increased neurons\n",
    "        self.bn1 = nn.BatchNorm1d(256)  # Batch normalization\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.fc5 = nn.Linear(32, 1)\n",
    "        self.dropout = nn.Dropout(p=0.2)  # Dropout to reduce overfitting\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))  # LeakyReLU activation\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_train(model_class, X_train, y_train, epochs, k_folds, patience=50):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "    best_overall_val_loss = float('inf')\n",
    "    best_overall_model_state = None\n",
    "    \n",
    "    # Convert to tensors if not already\n",
    "    if not isinstance(X_train, torch.Tensor):\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    if not isinstance(y_train, torch.Tensor):\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train)):\n",
    "        print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
    "        \n",
    "        # Use indices directly for splitting\n",
    "        X_fold_train = X_train[train_idx]\n",
    "        y_fold_train = y_train[train_idx]\n",
    "        X_val = X_train[val_idx]\n",
    "        y_val = y_train[val_idx]\n",
    "        \n",
    "        model = model_class(X_train.shape[1])\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=20, verbose=True\n",
    "        )\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        best_fold_val_loss = float('inf')\n",
    "        best_fold_model_state = None\n",
    "        patience_counter = 0\n",
    "        best_epoch_rmse = float('inf')\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_fold_train)\n",
    "            loss = criterion(outputs, y_fold_train)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(X_val)\n",
    "                val_loss = criterion(val_outputs, y_val)\n",
    "            \n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # Early stopping check\n",
    "            if val_loss < best_fold_val_loss:\n",
    "                best_fold_val_loss = val_loss\n",
    "                best_fold_model_state = model.state_dict()\n",
    "                patience_counter = 0\n",
    "                best_epoch_rmse = torch.sqrt(val_loss).item()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            train_rmse = torch.sqrt(loss).item()\n",
    "            val_rmse = torch.sqrt(val_loss).item()\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                print(f\"Best validation RMSE: {best_epoch_rmse:.4f}\")\n",
    "                break\n",
    "                \n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Train RMSE: {train_rmse:.4f}, \"\n",
    "                      f\"Val RMSE: {val_rmse:.4f}\")\n",
    "        \n",
    "        # Store fold results\n",
    "        fold_results.append(best_epoch_rmse)  # Store best RMSE instead of final\n",
    "        \n",
    "        if best_fold_val_loss < best_overall_val_loss:\n",
    "            best_overall_val_loss = best_fold_val_loss\n",
    "            best_overall_model_state = best_fold_model_state.copy()\n",
    "            print(f\"New best model found in fold {fold + 1}\")\n",
    "    \n",
    "    print(\"\\nCross-Validation Results:\")\n",
    "    print(f\"Fold RMSEs: {[f'{rmse:.4f}' for rmse in fold_results]}\")\n",
    "    print(f\"Mean RMSE: {np.mean(fold_results):.4f}\")\n",
    "    print(f\"Standard Deviation: {np.std(fold_results):.4f}\")\n",
    "    \n",
    "    # Save models\n",
    "    torch.save({\n",
    "        'model_state': best_overall_model_state,\n",
    "        'fold_results': fold_results,\n",
    "        'mean_rmse': np.mean(fold_results),\n",
    "        'std_rmse': np.std(fold_results)\n",
    "    }, \"best_model.pth\")\n",
    "    \n",
    "    return best_overall_model_state, fold_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/3\n",
      "Epoch [100/500], Train RMSE: 0.5380, Val RMSE: 0.5683\n",
      "Epoch [200/500], Train RMSE: 0.4762, Val RMSE: 0.5542\n",
      "Epoch [300/500], Train RMSE: 0.4469, Val RMSE: 0.5480\n",
      "Epoch [400/500], Train RMSE: 0.4336, Val RMSE: 0.5382\n",
      "Early stopping triggered at epoch 406\n",
      "Best validation RMSE: 0.5369\n",
      "New best model found in fold 1\n",
      "\n",
      "Fold 2/3\n",
      "Epoch [100/500], Train RMSE: 0.5230, Val RMSE: 0.5839\n",
      "Epoch [200/500], Train RMSE: 0.4775, Val RMSE: 0.5734\n",
      "Epoch [300/500], Train RMSE: 0.4465, Val RMSE: 0.5660\n",
      "Epoch [400/500], Train RMSE: 0.4331, Val RMSE: 0.5624\n",
      "Early stopping triggered at epoch 412\n",
      "Best validation RMSE: 0.5603\n",
      "\n",
      "Fold 3/3\n",
      "Epoch [100/500], Train RMSE: 0.5299, Val RMSE: 0.5902\n",
      "Epoch [200/500], Train RMSE: 0.5049, Val RMSE: 0.5735\n",
      "Epoch [300/500], Train RMSE: 0.4611, Val RMSE: 0.5491\n",
      "Epoch [400/500], Train RMSE: 0.4409, Val RMSE: 0.5430\n",
      "Epoch [500/500], Train RMSE: 0.4354, Val RMSE: 0.5400\n",
      "\n",
      "Cross-Validation Results:\n",
      "Fold RMSEs: ['0.5369', '0.5603', '0.5393']\n",
      "Mean RMSE: 0.5455\n",
      "Standard Deviation: 0.0105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('fc1.weight',\n",
       "               tensor([[ 6.3861e-03, -4.0482e-03,  5.7257e-03,  ..., -3.0550e-04,\n",
       "                        -2.1241e-03,  4.9828e-06],\n",
       "                       [ 1.0224e-03, -6.9111e-04, -1.3997e-02,  ...,  6.6900e-05,\n",
       "                         7.7249e-04, -6.9696e-04],\n",
       "                       [ 1.5664e-02, -8.3485e-03, -1.4089e-03,  ...,  1.7284e-04,\n",
       "                         2.0960e-04,  6.8606e-04],\n",
       "                       ...,\n",
       "                       [ 4.8816e-03, -1.3535e-03,  3.8351e-03,  ...,  9.3790e-04,\n",
       "                         9.5236e-04,  1.6580e-03],\n",
       "                       [-2.3832e-03,  2.6268e-03,  1.4924e-03,  ..., -1.0877e-03,\n",
       "                        -1.6152e-03, -1.4046e-03],\n",
       "                       [ 4.8556e-03, -5.4235e-03,  5.7462e-03,  ...,  2.1943e-04,\n",
       "                        -7.9379e-05, -3.9838e-04]])),\n",
       "              ('fc1.bias',\n",
       "               tensor([-1.7148e-08,  2.3676e-08, -1.0171e-08,  1.6042e-07, -1.8253e-07,\n",
       "                       -1.4486e-08, -1.0484e-09, -3.3464e-08, -6.3731e-09, -7.2302e-11,\n",
       "                        6.8211e-10, -1.7745e-08, -8.6516e-10, -6.2024e-08, -9.1779e-08,\n",
       "                       -1.2234e-07,  2.0466e-08,  2.6481e-09, -4.5394e-09,  3.5535e-07,\n",
       "                        2.9211e-09, -1.1893e-08, -1.0833e-08, -4.8050e-09,  7.1045e-10,\n",
       "                       -1.8647e-08, -1.2867e-08, -6.6672e-09,  3.7252e-08, -5.4915e-09,\n",
       "                        3.7153e-09, -1.2957e-09,  6.7956e-08,  3.7186e-10,  2.7683e-08,\n",
       "                       -3.5087e-08, -2.0326e-08, -5.4281e-08,  5.5530e-09, -8.6270e-09,\n",
       "                       -7.1769e-08,  7.0365e-09, -6.1350e-09,  1.3783e-10,  1.9408e-10,\n",
       "                        2.1608e-08,  1.2754e-08,  5.3752e-11, -1.2258e-08,  3.3303e-08,\n",
       "                       -6.0474e-08, -3.2509e-08, -8.7959e-11,  1.2571e-08,  7.4894e-09,\n",
       "                        4.6082e-08,  6.5708e-08, -6.6275e-09, -1.1194e-09, -1.2568e-08,\n",
       "                       -2.9617e-08,  1.4715e-08, -3.0587e-09, -2.2851e-08,  1.8714e-08,\n",
       "                       -1.5656e-08, -2.8544e-09,  1.4029e-08, -9.9596e-10, -1.1584e-07,\n",
       "                       -9.8510e-09,  2.5183e-08, -1.5518e-08, -6.6837e-08, -1.1995e-08,\n",
       "                        2.0233e-09,  3.2130e-08,  1.7951e-08, -1.7280e-08, -5.0713e-08,\n",
       "                        1.6787e-08,  1.4144e-08, -6.4488e-09, -5.5931e-09, -4.3883e-09,\n",
       "                       -5.7746e-09,  9.2600e-09,  8.2494e-10,  2.7526e-08, -2.2895e-09,\n",
       "                       -4.8392e-08, -3.6945e-09, -1.1012e-08,  2.6884e-09, -3.2711e-09,\n",
       "                        9.3439e-08,  2.3970e-08,  2.0264e-08,  4.6217e-09, -9.3164e-10,\n",
       "                        1.4465e-08,  1.2693e-08,  1.3604e-08, -5.0758e-08,  8.9707e-08,\n",
       "                       -3.1550e-08,  6.4355e-08, -1.0178e-08,  6.4316e-09,  5.9608e-09,\n",
       "                       -2.3037e-08,  2.0147e-08, -1.4935e-08,  3.4680e-09, -6.9976e-08,\n",
       "                       -1.3571e-10, -2.2000e-08,  5.1278e-08,  3.6796e-08, -1.6434e-10,\n",
       "                        2.4462e-09, -4.9571e-08, -8.4200e-10, -1.3677e-08,  2.6991e-09,\n",
       "                       -6.7727e-09, -1.7021e-08, -5.5253e-08, -2.8283e-11,  7.3033e-09,\n",
       "                        3.3820e-09,  4.0433e-09, -5.3152e-08, -1.7142e-08,  5.4347e-08,\n",
       "                       -1.4311e-09, -2.3306e-09,  3.9738e-09, -4.7667e-09,  6.5688e-08,\n",
       "                       -2.0073e-08, -5.5758e-08,  1.7407e-08, -6.3958e-09, -1.4750e-09,\n",
       "                       -1.9453e-09,  6.8568e-10, -9.1533e-09, -8.9900e-09, -8.6753e-09,\n",
       "                       -4.1283e-09, -2.2233e-09,  9.4880e-09,  9.1811e-09,  3.1492e-09,\n",
       "                        2.1423e-08, -1.8037e-08,  2.2898e-08, -2.1223e-09,  4.6136e-09,\n",
       "                       -5.8335e-09,  7.4173e-09, -4.0061e-08,  7.4151e-08,  3.5359e-09,\n",
       "                        1.2342e-08, -9.5013e-09, -2.3482e-09, -1.0216e-08, -2.7282e-07,\n",
       "                       -1.0150e-07,  2.5672e-08, -6.0466e-08,  3.2591e-08, -2.7087e-08,\n",
       "                       -4.0455e-09,  2.7828e-09,  9.3536e-09,  5.2866e-10,  3.3781e-09,\n",
       "                       -4.3139e-09,  1.6330e-09,  3.1306e-08,  5.8704e-08, -3.3843e-09,\n",
       "                       -4.9805e-08,  4.6442e-09, -1.0076e-08,  2.5895e-08,  1.2041e-07,\n",
       "                        6.9039e-09, -2.8696e-08, -3.2510e-08, -8.7290e-09,  1.8035e-08,\n",
       "                        1.2027e-09, -1.5967e-08,  2.0352e-08, -8.8556e-11, -1.2398e-08,\n",
       "                        2.3033e-08, -2.5416e-09,  1.6781e-08,  7.7213e-09, -8.6637e-08,\n",
       "                       -2.5230e-10, -2.4274e-10,  6.1259e-09,  1.3911e-08,  2.0871e-09,\n",
       "                       -6.3911e-09, -1.3455e-08,  2.4621e-09,  5.4234e-10,  6.9653e-10,\n",
       "                        1.1686e-09, -1.8561e-09, -1.5646e-08,  1.3389e-08,  6.8367e-09,\n",
       "                        1.4637e-09,  1.7813e-08, -1.4723e-08, -2.8766e-08, -7.3813e-09,\n",
       "                        1.8445e-08, -3.4579e-08,  2.4395e-08, -2.6270e-08, -1.1614e-07,\n",
       "                       -1.0782e-08,  6.4466e-08, -2.6428e-09, -2.3819e-08,  1.8916e-08,\n",
       "                        3.0156e-09,  3.0081e-10,  2.4950e-08,  2.9033e-09, -3.2066e-08,\n",
       "                       -1.0232e-08, -1.3495e-09,  1.7634e-08, -1.8199e-08,  4.8064e-08,\n",
       "                        6.4187e-09,  1.3719e-08, -2.6524e-08,  7.8418e-08,  5.8913e-11,\n",
       "                       -2.3817e-08,  8.1182e-08, -3.4684e-09,  2.6328e-09,  1.1310e-08,\n",
       "                        1.5899e-09])),\n",
       "              ('bn1.weight',\n",
       "               tensor([0.7949, 0.7958, 0.7948, 0.7953, 0.7957, 0.7947, 0.7953, 0.7961, 0.7951,\n",
       "                       0.7947, 0.7946, 0.7949, 0.7950, 0.7956, 0.7959, 0.7958, 0.7942, 0.7948,\n",
       "                       0.7953, 0.7951, 0.7952, 0.7957, 0.7951, 0.7953, 0.7946, 0.7949, 0.7950,\n",
       "                       0.7949, 0.7949, 0.7957, 0.7944, 0.7947, 0.7951, 0.7951, 0.7959, 0.7951,\n",
       "                       0.7941, 0.7954, 0.7956, 0.7946, 0.7962, 0.7950, 0.7955, 0.7954, 0.7948,\n",
       "                       0.7945, 0.7950, 0.7951, 0.7949, 0.7952, 0.7957, 0.7949, 0.7949, 0.7959,\n",
       "                       0.7955, 0.7950, 0.7944, 0.7952, 0.7950, 0.7960, 0.7942, 0.7951, 0.7946,\n",
       "                       0.7957, 0.7949, 0.7954, 0.7942, 0.7949, 0.7954, 0.7950, 0.7947, 0.7950,\n",
       "                       0.7947, 0.7955, 0.7942, 0.7952, 0.7951, 0.7944, 0.7952, 0.7948, 0.7954,\n",
       "                       0.7954, 0.7931, 0.7957, 0.7950, 0.7957, 0.7944, 0.7948, 0.7948, 0.7950,\n",
       "                       0.7956, 0.7953, 0.7951, 0.7954, 0.7950, 0.7953, 0.7949, 0.7941, 0.7954,\n",
       "                       0.7953, 0.7955, 0.7954, 0.7955, 0.7955, 0.7959, 0.7945, 0.7949, 0.7955,\n",
       "                       0.7949, 0.7950, 0.7944, 0.7949, 0.7944, 0.7951, 0.7955, 0.7948, 0.7950,\n",
       "                       0.7958, 0.7950, 0.7950, 0.7946, 0.7935, 0.7949, 0.7956, 0.7952, 0.7951,\n",
       "                       0.7949, 0.7950, 0.7949, 0.7955, 0.7945, 0.7946, 0.7943, 0.7954, 0.7944,\n",
       "                       0.7962, 0.7955, 0.7957, 0.7951, 0.7952, 0.7951, 0.7951, 0.7953, 0.7948,\n",
       "                       0.7947, 0.7953, 0.7955, 0.7959, 0.7949, 0.7951, 0.7952, 0.7947, 0.7952,\n",
       "                       0.7952, 0.7943, 0.7967, 0.7953, 0.7951, 0.7947, 0.7958, 0.7952, 0.7954,\n",
       "                       0.7955, 0.7953, 0.7954, 0.7955, 0.7941, 0.7951, 0.7957, 0.7948, 0.7948,\n",
       "                       0.7953, 0.7946, 0.7949, 0.7961, 0.7939, 0.7951, 0.7953, 0.7957, 0.7944,\n",
       "                       0.7944, 0.7948, 0.7951, 0.7966, 0.7958, 0.7952, 0.7947, 0.7953, 0.7954,\n",
       "                       0.7958, 0.7944, 0.7956, 0.7949, 0.7949, 0.7952, 0.7948, 0.7956, 0.7953,\n",
       "                       0.7949, 0.7941, 0.7951, 0.7951, 0.7956, 0.7955, 0.7956, 0.7948, 0.7947,\n",
       "                       0.7952, 0.7953, 0.7961, 0.7950, 0.7955, 0.7961, 0.7962, 0.7946, 0.7949,\n",
       "                       0.7946, 0.7953, 0.7953, 0.7955, 0.7957, 0.7952, 0.7949, 0.7955, 0.7950,\n",
       "                       0.7952, 0.7960, 0.7953, 0.7955, 0.7953, 0.7956, 0.7952, 0.7946, 0.7962,\n",
       "                       0.7949, 0.7948, 0.7951, 0.7952, 0.7953, 0.7955, 0.7947, 0.7943, 0.7956,\n",
       "                       0.7951, 0.7952, 0.7958, 0.7952, 0.7952, 0.7950, 0.7951, 0.7950, 0.7948,\n",
       "                       0.7950, 0.7950, 0.7955, 0.7949])),\n",
       "              ('bn1.bias',\n",
       "               tensor([-1.3824e-02, -1.4023e-02, -5.7477e-03, -1.2053e-02, -1.8250e-02,\n",
       "                       -8.0512e-03, -1.3310e-02, -2.9909e-02, -8.9542e-03, -1.4510e-02,\n",
       "                       -1.4923e-03, -1.0056e-02, -2.0883e-03, -1.9513e-02, -2.5712e-02,\n",
       "                       -2.7316e-02, -4.3712e-02, -1.2734e-03, -7.7898e-03, -1.2773e-02,\n",
       "                       -1.0175e-02, -2.6306e-02, -5.8885e-03, -8.3372e-03, -2.2080e-02,\n",
       "                       -3.0444e-02, -1.7814e-02, -9.7207e-03, -1.8441e-02, -5.7651e-03,\n",
       "                       -1.5626e-02, -1.2405e-02, -1.2044e-02, -6.1483e-04, -6.9404e-03,\n",
       "                       -1.6541e-02, -1.6246e-02, -1.1984e-02, -1.6578e-02, -6.5639e-03,\n",
       "                       -2.6415e-02, -8.9750e-03, -1.0616e-02, -8.7688e-03,  6.0910e-05,\n",
       "                       -1.1828e-02, -1.8598e-02, -4.5514e-03, -1.2596e-02, -2.8814e-02,\n",
       "                       -3.4551e-02, -3.0746e-02, -7.8844e-03, -2.9817e-02, -3.5015e-03,\n",
       "                       -8.9013e-03, -2.5503e-02, -1.7858e-02, -3.9026e-03, -1.2876e-02,\n",
       "                       -1.2245e-02, -1.3933e-02, -1.1007e-02, -1.1901e-02, -1.2060e-02,\n",
       "                       -2.4553e-02, -4.2401e-03, -5.3190e-03, -1.0177e-02, -2.3114e-02,\n",
       "                       -1.6809e-03, -2.5286e-02, -1.5993e-02, -9.3596e-03, -8.5105e-03,\n",
       "                       -2.0684e-02, -2.1332e-02, -2.0649e-02, -1.3375e-02, -1.6443e-02,\n",
       "                       -1.9426e-02, -6.7192e-03, -8.2588e-03, -6.6698e-03, -2.0067e-02,\n",
       "                       -1.6090e-02, -1.1981e-02, -3.0934e-02, -2.4538e-02, -1.0882e-02,\n",
       "                       -1.8433e-02, -1.1387e-02, -7.5973e-03, -9.5978e-03, -3.7454e-03,\n",
       "                       -1.2378e-02, -2.1076e-02, -1.8740e-02, -9.7851e-03, -2.9830e-03,\n",
       "                       -7.5186e-02, -3.6774e-03, -1.5802e-02, -2.8727e-02, -2.9700e-02,\n",
       "                       -2.0556e-02, -1.1297e-02, -7.3054e-03, -2.7553e-03, -1.5550e-02,\n",
       "                       -1.6358e-02, -3.2392e-02, -3.1054e-02, -5.3470e-03, -3.3589e-02,\n",
       "                       -1.0367e-02, -2.4010e-02, -2.6521e-02, -4.0681e-02, -2.3921e-03,\n",
       "                       -2.1262e-03, -1.4402e-02, -1.2353e-02, -2.3276e-02, -5.9265e-03,\n",
       "                       -1.2096e-02, -6.6181e-03, -2.3846e-02, -6.5484e-03, -9.7164e-03,\n",
       "                       -1.3735e-02, -2.5420e-02, -1.7107e-02, -3.9013e-02, -2.3001e-02,\n",
       "                       -1.0204e-02, -6.9909e-03, -7.2000e-03, -2.3095e-02, -2.3218e-02,\n",
       "                       -9.0291e-03, -1.8196e-02, -4.8183e-03, -1.2940e-02, -4.9031e-03,\n",
       "                       -3.0346e-03, -9.3662e-03, -2.3776e-02, -1.5862e-02, -3.6375e-03,\n",
       "                       -6.8391e-03, -1.2445e-03, -8.2115e-03, -2.5761e-02, -6.8750e-03,\n",
       "                       -1.5015e-03, -1.4504e-02, -3.9791e-03, -4.3298e-03, -7.5157e-03,\n",
       "                       -5.9196e-03, -5.2822e-03, -8.7290e-03, -1.1838e-02, -2.2792e-02,\n",
       "                       -6.4894e-03, -2.5380e-02, -2.0829e-02, -1.0954e-02, -2.6272e-02,\n",
       "                       -3.1556e-02, -5.1890e-03, -1.1493e-02, -3.0620e-02, -1.1577e-02,\n",
       "                       -1.1678e-02, -5.1469e-04, -2.6506e-02, -4.3065e-04, -4.3467e-03,\n",
       "                       -7.5880e-03, -2.4456e-02, -5.2407e-03, -1.0913e-02, -1.0574e-02,\n",
       "                       -2.8044e-03, -1.3736e-02, -2.0786e-02, -1.6903e-02, -5.1680e-02,\n",
       "                       -2.2004e-03, -1.7275e-02, -4.2766e-02, -1.2594e-02, -8.0463e-03,\n",
       "                       -1.2399e-02, -3.4201e-02, -2.4576e-02, -8.4188e-03, -1.9610e-02,\n",
       "                       -1.3213e-02, -1.4244e-02, -6.5842e-03, -9.7702e-03, -1.7945e-02,\n",
       "                       -1.0702e-03, -2.3362e-02, -8.9769e-03, -2.2566e-02, -1.7854e-02,\n",
       "                       -3.2572e-03, -6.6422e-03, -1.1616e-02, -1.7011e-03, -2.5459e-03,\n",
       "                       -1.3864e-02, -5.5164e-03, -5.9809e-03, -1.1929e-02, -1.7133e-02,\n",
       "                       -1.4923e-03, -2.3104e-02, -1.6659e-02, -1.8485e-02, -1.7169e-02,\n",
       "                       -1.8494e-02, -2.3694e-02, -2.0785e-02, -1.0225e-02, -2.0602e-02,\n",
       "                       -1.5589e-02, -2.0911e-03, -5.4206e-03,  4.6062e-04, -1.2532e-02,\n",
       "                       -2.4951e-02, -8.4040e-03, -1.1266e-02, -1.1057e-02, -1.1000e-02,\n",
       "                       -2.7267e-03, -1.0859e-02, -2.1575e-02, -8.7405e-04, -1.5713e-02,\n",
       "                       -4.9806e-04, -2.2052e-02, -2.3088e-02, -2.4150e-02, -1.3401e-02,\n",
       "                       -8.8848e-03, -7.1847e-03, -7.3926e-03, -9.7347e-03, -3.2840e-03,\n",
       "                       -3.6341e-03])),\n",
       "              ('bn1.running_mean',\n",
       "               tensor([ 4.9991e-04, -1.4304e-03, -6.0673e-04, -6.4805e-04, -1.0251e-04,\n",
       "                        1.8942e-03, -2.7006e-04, -1.8728e-03, -5.3547e-04, -2.9136e-04,\n",
       "                       -3.8786e-03,  2.7737e-04, -2.9623e-03, -5.0467e-04, -4.7463e-04,\n",
       "                       -8.8001e-05,  2.3952e-03, -8.6389e-04,  9.0412e-04, -1.2928e-03,\n",
       "                        1.1911e-03, -1.1296e-03,  4.3238e-04,  1.4533e-03,  2.0295e-03,\n",
       "                       -4.7647e-04, -1.3628e-03, -2.3691e-03, -1.4906e-03, -1.3964e-04,\n",
       "                        1.0376e-03,  4.6912e-04, -1.8529e-03, -3.8183e-03,  5.5695e-04,\n",
       "                       -4.5762e-04,  1.1600e-03, -6.0113e-04,  1.2117e-02,  6.6468e-04,\n",
       "                       -9.1711e-06, -8.0985e-04, -8.3475e-04, -9.4986e-03, -2.0559e-03,\n",
       "                       -2.1699e-04,  6.2105e-05, -3.2215e-03, -1.2476e-03, -1.5825e-03,\n",
       "                       -3.8549e-05, -3.9357e-04, -4.7896e-03, -6.6735e-04, -2.8553e-04,\n",
       "                       -3.2493e-03,  8.6353e-04, -2.2832e-03, -1.1581e-03,  1.2473e-03,\n",
       "                       -1.7515e-04, -9.0013e-04,  2.5946e-03,  8.4151e-04,  7.5359e-04,\n",
       "                       -5.4225e-04, -2.8222e-03, -9.8259e-04, -2.2968e-03,  6.0546e-04,\n",
       "                       -2.7598e-03,  4.4274e-04, -1.4787e-03, -1.9202e-03,  1.4209e-03,\n",
       "                       -2.3139e-03, -7.2901e-04, -3.0527e-03,  2.0490e-04,  4.3457e-04,\n",
       "                        7.9010e-04,  4.9237e-04, -1.4052e-03, -1.0096e-03,  2.3490e-05,\n",
       "                        4.6032e-04,  7.8168e-04, -1.4786e-03,  7.3158e-04, -1.4162e-03,\n",
       "                       -1.3810e-03, -7.4335e-04, -1.5877e-03,  1.5745e-03,  3.1783e-03,\n",
       "                        1.5603e-04, -2.7105e-03,  3.8302e-03,  2.5997e-05, -1.9389e-03,\n",
       "                       -7.1157e-04, -2.2023e-03, -1.6552e-03, -3.1883e-03, -1.3127e-03,\n",
       "                       -5.5625e-04, -1.6567e-03, -3.0723e-04, -4.6406e-04,  1.8384e-03,\n",
       "                        1.7794e-03, -5.1614e-04, -2.8032e-03, -4.5072e-03, -1.1895e-04,\n",
       "                        2.8871e-04,  1.3415e-03, -3.3861e-04,  2.2806e-04, -1.5489e-03,\n",
       "                        8.5257e-04, -1.0796e-03, -5.9751e-04, -1.0135e-03, -2.5129e-03,\n",
       "                        1.5685e-03, -2.2222e-03, -2.9441e-03,  5.5805e-03, -2.6351e-04,\n",
       "                       -4.0604e-04, -2.2984e-03, -1.2840e-03, -2.7768e-03, -1.3487e-03,\n",
       "                       -1.7900e-03, -1.4295e-03,  1.9656e-04,  1.9831e-04, -6.4799e-05,\n",
       "                       -1.5243e-03, -1.7492e-04,  2.5059e-03, -3.0945e-03, -2.6448e-03,\n",
       "                       -2.6296e-03, -1.1575e-03, -2.7591e-03,  1.0419e-03,  1.4346e-03,\n",
       "                       -1.0323e-03, -2.4465e-03, -7.7578e-04,  6.4734e-04, -2.1137e-03,\n",
       "                        2.8678e-04,  1.9202e-04,  1.6959e-03, -3.4843e-03, -9.6196e-04,\n",
       "                       -2.1225e-03,  1.8169e-03,  3.5981e-04,  5.6434e-04,  4.1900e-04,\n",
       "                       -5.8874e-04, -3.8248e-03, -1.4389e-03, -2.9223e-04, -9.8740e-04,\n",
       "                        1.0106e-03, -1.3643e-03,  4.6015e-03, -5.4425e-04, -1.5804e-04,\n",
       "                       -2.6470e-03, -4.9968e-04,  1.7750e-03, -2.4469e-03,  6.8478e-04,\n",
       "                       -1.9458e-03, -4.0754e-04, -2.4775e-04, -1.0187e-03, -7.0810e-03,\n",
       "                        2.3133e-03, -1.6467e-03,  9.8939e-04, -2.5040e-03, -4.8815e-04,\n",
       "                       -1.5245e-03, -2.2250e-04,  1.8734e-04, -2.7598e-04, -1.4731e-03,\n",
       "                        1.0508e-04, -5.4370e-04, -1.6480e-04,  9.0537e-04,  1.2571e-03,\n",
       "                        3.1303e-04, -3.5514e-03,  3.8761e-04, -1.4251e-03, -1.5713e-04,\n",
       "                       -2.0688e-03, -3.4438e-04,  9.9987e-05, -2.7722e-05,  5.7320e-04,\n",
       "                        2.2096e-03, -9.2509e-04, -7.0065e-04, -2.5497e-03, -3.8505e-03,\n",
       "                        3.4527e-03, -1.5668e-03,  6.0786e-03, -5.3557e-03,  4.8495e-04,\n",
       "                       -1.6960e-03, -1.0431e-03,  5.1683e-04,  8.9999e-04,  1.6107e-04,\n",
       "                        3.2043e-05,  3.6671e-04,  1.6549e-04,  3.6273e-04, -1.0542e-03,\n",
       "                       -1.0744e-03, -1.9981e-04, -5.5307e-05,  2.0380e-04, -2.9682e-04,\n",
       "                        7.6255e-04, -1.6516e-03, -2.2006e-03, -4.6386e-04, -1.8567e-04,\n",
       "                       -7.6857e-04, -1.0032e-04, -3.2613e-04, -6.6310e-04, -1.7176e-04,\n",
       "                       -1.5375e-03,  1.2916e-03,  8.5978e-04,  9.0988e-04, -4.8722e-04,\n",
       "                        2.6712e-03,  1.4750e-05, -2.4944e-03, -2.3667e-03, -6.9231e-04,\n",
       "                       -3.4896e-04])),\n",
       "              ('bn1.running_var',\n",
       "               tensor([0.0084, 0.0117, 0.0080, 0.0095, 0.0038, 0.0188, 0.0086, 0.0199, 0.0091,\n",
       "                       0.0175, 0.0508, 0.0045, 0.0280, 0.0187, 0.0066, 0.0131, 0.0093, 0.0081,\n",
       "                       0.0082, 0.0082, 0.0187, 0.0107, 0.0042, 0.0106, 0.0080, 0.0123, 0.0238,\n",
       "                       0.0204, 0.0100, 0.0128, 0.0160, 0.0103, 0.0091, 0.0706, 0.0070, 0.0055,\n",
       "                       0.0097, 0.0035, 0.4228, 0.0039, 0.0110, 0.0065, 0.0084, 0.3109, 0.0153,\n",
       "                       0.0048, 0.0090, 0.0458, 0.0178, 0.0148, 0.0106, 0.0062, 0.0575, 0.0073,\n",
       "                       0.0038, 0.0187, 0.0196, 0.0293, 0.0091, 0.0138, 0.0065, 0.0041, 0.0231,\n",
       "                       0.0072, 0.0189, 0.0109, 0.0255, 0.0046, 0.0447, 0.0123, 0.0272, 0.0087,\n",
       "                       0.0111, 0.0156, 0.0094, 0.0181, 0.0088, 0.0369, 0.0082, 0.0052, 0.0100,\n",
       "                       0.0031, 0.0164, 0.0114, 0.0175, 0.0081, 0.0100, 0.0114, 0.0040, 0.0083,\n",
       "                       0.0060, 0.0104, 0.0119, 0.0109, 0.0427, 0.0046, 0.0079, 0.0759, 0.0061,\n",
       "                       0.0301, 0.0171, 0.0099, 0.0141, 0.0090, 0.0082, 0.0045, 0.0140, 0.0052,\n",
       "                       0.0056, 0.0089, 0.0164, 0.0166, 0.0162, 0.0463, 0.0087, 0.0043, 0.0095,\n",
       "                       0.0098, 0.0051, 0.0293, 0.0071, 0.0246, 0.0056, 0.0092, 0.0260, 0.0088,\n",
       "                       0.0245, 0.0128, 0.0946, 0.0075, 0.0064, 0.0171, 0.0092, 0.0553, 0.0126,\n",
       "                       0.0206, 0.0168, 0.0131, 0.0095, 0.0062, 0.0194, 0.0163, 0.0143, 0.0112,\n",
       "                       0.0050, 0.0189, 0.0199, 0.0168, 0.0150, 0.0112, 0.0145, 0.0229, 0.0116,\n",
       "                       0.0089, 0.0095, 0.0087, 0.0084, 0.0097, 0.0361, 0.0159, 0.0064, 0.0091,\n",
       "                       0.0093, 0.0055, 0.0093, 0.0053, 0.0289, 0.0107, 0.0134, 0.0070, 0.0182,\n",
       "                       0.0045, 0.0645, 0.0158, 0.0055, 0.0162, 0.0112, 0.0057, 0.0236, 0.0054,\n",
       "                       0.0183, 0.0075, 0.0089, 0.0081, 0.1444, 0.0272, 0.0091, 0.0166, 0.0185,\n",
       "                       0.0130, 0.0133, 0.0135, 0.0184, 0.0127, 0.0105, 0.0150, 0.0116, 0.0159,\n",
       "                       0.0140, 0.0195, 0.0093, 0.0392, 0.0040, 0.0096, 0.0078, 0.0178, 0.0140,\n",
       "                       0.0062, 0.0161, 0.0057, 0.0174, 0.0040, 0.0071, 0.0170, 0.0613, 0.0365,\n",
       "                       0.0104, 0.1170, 0.0700, 0.0067, 0.0151, 0.0191, 0.0110, 0.0110, 0.0125,\n",
       "                       0.0100, 0.0062, 0.0100, 0.0060, 0.0056, 0.0068, 0.0067, 0.0053, 0.0103,\n",
       "                       0.0232, 0.0096, 0.0071, 0.0244, 0.0063, 0.0136, 0.0097, 0.0064, 0.0037,\n",
       "                       0.0104, 0.0065, 0.0132, 0.0104, 0.0157, 0.0080, 0.0099, 0.0459, 0.0081,\n",
       "                       0.0318, 0.0237, 0.0131, 0.0044])),\n",
       "              ('bn1.num_batches_tracked', tensor(407)),\n",
       "              ('fc2.weight',\n",
       "               tensor([[ 8.8289e-03, -1.4179e-02,  4.7982e-03,  ...,  3.4405e-03,\n",
       "                         4.8309e-03, -2.1786e-03],\n",
       "                       [-4.2123e-04, -1.9767e-03,  7.0641e-04,  ..., -1.4473e-03,\n",
       "                         5.4962e-03, -8.2452e-04],\n",
       "                       [-5.5266e-03,  3.1888e-03,  5.1791e-03,  ..., -2.5174e-05,\n",
       "                        -4.9092e-03, -9.4228e-03],\n",
       "                       ...,\n",
       "                       [-6.2555e-04,  4.7283e-03, -6.4460e-03,  ...,  2.5068e-03,\n",
       "                        -4.7295e-04,  4.3020e-03],\n",
       "                       [ 7.1816e-03,  3.7393e-04,  3.6553e-03,  ...,  3.7354e-03,\n",
       "                        -6.3188e-03, -5.8148e-04],\n",
       "                       [-9.3421e-05, -2.2670e-03,  2.2760e-03,  ...,  5.1128e-03,\n",
       "                        -2.4988e-03,  1.5114e-03]])),\n",
       "              ('fc2.bias',\n",
       "               tensor([-1.0239e-09,  1.5406e-08, -1.6232e-08,  3.8751e-08, -2.6555e-08,\n",
       "                        1.2515e-08,  6.3118e-09,  5.4112e-09, -1.2394e-08,  1.1005e-08,\n",
       "                        3.0337e-07,  2.9379e-08,  3.1874e-08,  3.3547e-08, -3.3723e-08,\n",
       "                       -2.9375e-09, -9.3218e-10, -9.3437e-08,  6.3606e-08,  3.7801e-07,\n",
       "                        2.6048e-08,  5.5546e-09,  3.2142e-08, -5.6793e-07,  2.6839e-08,\n",
       "                       -1.6326e-09, -9.8158e-08,  4.2763e-08, -2.9381e-08,  3.2244e-08,\n",
       "                       -5.0747e-08,  7.8383e-09, -1.7028e-08,  7.4833e-08,  2.0213e-08,\n",
       "                        3.7724e-08,  4.8403e-09,  2.0852e-08,  6.5047e-07,  1.8771e-08,\n",
       "                       -1.1393e-07, -6.1261e-07,  8.5229e-08,  1.4086e-08, -2.1403e-08,\n",
       "                       -3.1165e-08, -1.0165e-07, -3.1320e-08,  3.1430e-08, -3.5190e-08,\n",
       "                       -2.6759e-08,  6.2560e-09,  8.2988e-09,  4.1852e-08, -7.5701e-07,\n",
       "                       -2.3756e-09, -3.3972e-08,  1.4682e-08,  1.9147e-08, -6.5108e-07,\n",
       "                        5.0880e-07, -4.8556e-07, -1.3017e-07,  3.2575e-09, -6.3779e-07,\n",
       "                        6.7679e-08,  2.4085e-08, -2.7962e-07,  7.1921e-09,  4.0808e-08,\n",
       "                        5.7110e-09,  5.2649e-08,  1.2358e-08, -2.8092e-08, -2.6392e-08,\n",
       "                       -1.8305e-08, -4.5465e-08, -4.5131e-07,  7.1171e-07, -9.6933e-09,\n",
       "                       -6.5575e-08,  3.5609e-07, -4.1816e-08,  3.3534e-08, -7.7029e-07,\n",
       "                       -1.0667e-07,  6.0027e-08, -1.3527e-08, -1.6787e-08,  3.2476e-08,\n",
       "                        1.5605e-08, -7.6455e-09, -2.0102e-08,  2.0936e-07,  4.5949e-08,\n",
       "                       -6.5121e-09,  4.5032e-07, -3.0041e-08, -2.7181e-07, -2.3861e-09,\n",
       "                       -1.2499e-08,  2.0483e-08,  5.1525e-10,  8.9431e-09, -3.2633e-08,\n",
       "                       -2.0028e-08,  2.7361e-08, -5.7453e-09, -4.7835e-08,  4.4523e-09,\n",
       "                        5.2769e-08,  1.6184e-07, -1.3813e-08,  2.0392e-08,  2.2855e-08,\n",
       "                        1.4211e-08, -7.7908e-09,  1.9692e-09,  1.1049e-08, -9.8583e-09,\n",
       "                        2.0277e-08, -2.8949e-07, -3.0483e-07, -1.3506e-08, -5.4073e-08,\n",
       "                        1.7749e-08, -2.5526e-08, -2.1540e-09])),\n",
       "              ('bn2.weight',\n",
       "               tensor([0.7963, 0.7954, 0.7951, 0.7949, 0.7957, 0.7929, 0.7940, 0.7945, 0.7963,\n",
       "                       0.7957, 0.7945, 0.7955, 0.7985, 0.7957, 0.7945, 0.7960, 0.7955, 0.7937,\n",
       "                       0.7957, 0.7960, 0.7951, 0.7953, 0.7948, 0.7953, 0.7951, 0.7956, 0.7960,\n",
       "                       0.7964, 0.7959, 0.7950, 0.7931, 0.7951, 0.7949, 0.7948, 0.7956, 0.7947,\n",
       "                       0.7954, 0.7950, 0.7950, 0.7953, 0.7952, 0.7959, 0.7951, 0.7952, 0.7947,\n",
       "                       0.7955, 0.7945, 0.7953, 0.7956, 0.7953, 0.7950, 0.7974, 0.7951, 0.7961,\n",
       "                       0.7943, 0.7939, 0.7945, 0.7933, 0.7954, 0.7932, 0.7941, 0.7957, 0.7956,\n",
       "                       0.7935, 0.7961, 0.7951, 0.7930, 0.7949, 0.7956, 0.7951, 0.7953, 0.7964,\n",
       "                       0.7948, 0.7968, 0.7961, 0.7953, 0.7952, 0.7945, 0.7931, 0.7958, 0.7967,\n",
       "                       0.7947, 0.7969, 0.7956, 0.7963, 0.7959, 0.7949, 0.7956, 0.7977, 0.7948,\n",
       "                       0.7959, 0.7951, 0.7946, 0.7955, 0.7944, 0.7944, 0.7936, 0.7957, 0.7957,\n",
       "                       0.7968, 0.7955, 0.7954, 0.7950, 0.7950, 0.7956, 0.7946, 0.7956, 0.7946,\n",
       "                       0.7949, 0.7951, 0.7956, 0.7956, 0.7959, 0.7952, 0.7955, 0.7946, 0.7949,\n",
       "                       0.7951, 0.7965, 0.7946, 0.7954, 0.7962, 0.7963, 0.7962, 0.7950, 0.7948,\n",
       "                       0.7963, 0.7951])),\n",
       "              ('bn2.bias',\n",
       "               tensor([-0.0248, -0.0170, -0.0146, -0.0128, -0.0159, -0.0091, -0.0218, -0.0152,\n",
       "                       -0.0196, -0.0121, -0.0115, -0.0075, -0.0294, -0.0089, -0.0121, -0.0039,\n",
       "                       -0.0216, -0.0201, -0.0265, -0.0225, -0.0139, -0.0332, -0.0134, -0.0069,\n",
       "                       -0.0194, -0.0198, -0.0094, -0.0292, -0.0291, -0.0248, -0.0046, -0.0318,\n",
       "                       -0.0189, -0.0072, -0.0117, -0.0178, -0.0330, -0.0147, -0.0172, -0.0161,\n",
       "                       -0.0236, -0.0189, -0.0190, -0.0307, -0.0034, -0.0045, -0.0216, -0.0221,\n",
       "                       -0.0016, -0.0223, -0.0057, -0.0083, -0.0121, -0.0219, -0.0173, -0.0095,\n",
       "                       -0.0107, -0.0224, -0.0080, -0.0055, -0.0086, -0.0102, -0.0101, -0.0088,\n",
       "                       -0.0054, -0.0155, -0.0306, -0.0323, -0.0061, -0.0384, -0.0112, -0.0104,\n",
       "                       -0.0276, -0.0113, -0.0257, -0.0167, -0.0194, -0.0113, -0.0203, -0.0157,\n",
       "                       -0.0079, -0.0139, -0.0075, -0.0330, -0.0079, -0.0152, -0.0074, -0.0317,\n",
       "                       -0.0298, -0.0080, -0.0214, -0.0162, -0.0109, -0.0207, -0.0138, -0.0061,\n",
       "                       -0.0264, -0.0220, -0.0135, -0.0002, -0.0367, -0.0178, -0.0164, -0.0129,\n",
       "                       -0.0133, -0.0318, -0.0290, -0.0084, -0.0315, -0.0404,  0.0003, -0.0231,\n",
       "                       -0.0088, -0.0192, -0.0313, -0.0190, -0.0055, -0.0273, -0.0249, -0.0226,\n",
       "                       -0.0213, -0.0060, -0.0228, -0.0283, -0.0248, -0.0060, -0.0096, -0.0173])),\n",
       "              ('bn2.running_mean',\n",
       "               tensor([-4.0576e-02,  2.8709e-02, -5.7658e-02,  4.3557e-02, -1.4894e-02,\n",
       "                        4.0495e-02, -4.2940e-02,  8.7441e-03, -7.3030e-03, -5.5259e-03,\n",
       "                        1.7145e-02,  5.2200e-02, -1.8330e-02, -5.2121e-03, -2.7572e-02,\n",
       "                        6.2462e-02, -7.6057e-02, -2.5579e-02, -5.1626e-02,  1.8380e-03,\n",
       "                        7.3860e-03, -1.6971e-02,  1.7855e-03, -2.7001e-02, -5.8020e-02,\n",
       "                       -6.0668e-02, -3.8733e-02, -3.8510e-03, -4.9052e-02, -1.8689e-02,\n",
       "                       -5.8085e-04, -2.8246e-02,  1.1266e-02, -9.9888e-04, -3.9093e-02,\n",
       "                       -3.4095e-02,  4.1325e-03,  1.3157e-02, -1.0665e-02, -1.0699e-02,\n",
       "                       -2.6039e-02, -4.1271e-03, -1.5041e-02, -2.9718e-02, -9.9377e-03,\n",
       "                        5.1193e-02, -4.7981e-03, -7.1540e-02,  4.6441e-04, -4.7623e-02,\n",
       "                        5.2578e-03, -4.2704e-02, -5.5776e-02, -2.9669e-02,  1.3900e-02,\n",
       "                       -9.6058e-03,  1.0524e-02,  4.0430e-02,  9.0586e-02,  2.6074e-02,\n",
       "                        2.4861e-02,  4.6415e-02,  1.2149e-03,  1.0586e-02,  6.3597e-03,\n",
       "                       -3.4785e-02,  1.9884e-02,  4.1217e-03,  7.7854e-03, -2.9158e-02,\n",
       "                        6.1527e-02, -4.1288e-02, -1.4938e-02,  5.3373e-05, -3.3279e-02,\n",
       "                       -2.5470e-02, -4.4466e-02, -7.8580e-03,  3.6839e-02, -1.7356e-02,\n",
       "                       -2.9317e-02,  5.2479e-03,  6.2689e-02, -7.0758e-03, -5.7237e-02,\n",
       "                       -3.8836e-02, -1.6721e-02, -3.4396e-02,  1.5624e-02,  6.5398e-02,\n",
       "                       -4.6685e-02,  8.1776e-03, -4.0743e-04, -3.4240e-02,  4.3920e-02,\n",
       "                        2.5485e-02, -5.1674e-02,  1.4623e-02,  5.9528e-02,  3.3845e-02,\n",
       "                       -1.8142e-02, -1.6136e-02, -1.0482e-02,  7.8005e-03,  1.0477e-02,\n",
       "                       -5.5851e-02, -4.5089e-02, -3.8015e-03, -2.4769e-02, -1.7542e-02,\n",
       "                        6.0246e-02, -2.4474e-02, -4.0402e-02, -2.0161e-02, -1.6927e-02,\n",
       "                        4.7805e-02, -6.7011e-05, -3.2766e-02, -1.9586e-02, -2.1213e-02,\n",
       "                       -3.1438e-02,  5.1667e-03, -1.4779e-02, -2.4252e-02, -3.3362e-03,\n",
       "                        9.1350e-02, -2.1036e-02,  2.0976e-04])),\n",
       "              ('bn2.running_var',\n",
       "               tensor([0.0049, 0.0080, 0.0090, 0.0077, 0.0039, 0.0088, 0.0090, 0.0046, 0.0035,\n",
       "                       0.0047, 0.0032, 0.0034, 0.0081, 0.0016, 0.0031, 0.0041, 0.0038, 0.0045,\n",
       "                       0.0063, 0.0038, 0.0087, 0.0063, 0.0040, 0.0024, 0.0131, 0.0044, 0.0024,\n",
       "                       0.0034, 0.0043, 0.0036, 0.0035, 0.0081, 0.0050, 0.0067, 0.0026, 0.0032,\n",
       "                       0.0059, 0.0038, 0.0030, 0.0041, 0.0094, 0.0058, 0.0035, 0.0039, 0.0015,\n",
       "                       0.0034, 0.0066, 0.0051, 0.0029, 0.0061, 0.0035, 0.0028, 0.0038, 0.0050,\n",
       "                       0.0037, 0.0074, 0.0077, 0.0078, 0.0065, 0.0070, 0.0077, 0.0072, 0.0028,\n",
       "                       0.0042, 0.0034, 0.0050, 0.0042, 0.0065, 0.0036, 0.0146, 0.0079, 0.0055,\n",
       "                       0.0091, 0.0032, 0.0039, 0.0027, 0.0057, 0.0023, 0.0073, 0.0073, 0.0030,\n",
       "                       0.0031, 0.0051, 0.0065, 0.0043, 0.0031, 0.0046, 0.0046, 0.0107, 0.0069,\n",
       "                       0.0057, 0.0049, 0.0020, 0.0042, 0.0080, 0.0061, 0.0107, 0.0092, 0.0048,\n",
       "                       0.0027, 0.0061, 0.0027, 0.0014, 0.0025, 0.0041, 0.0087, 0.0054, 0.0031,\n",
       "                       0.0065, 0.0171, 0.0035, 0.0045, 0.0027, 0.0063, 0.0088, 0.0076, 0.0027,\n",
       "                       0.0034, 0.0073, 0.0067, 0.0041, 0.0017, 0.0038, 0.0061, 0.0028, 0.0061,\n",
       "                       0.0048, 0.0044])),\n",
       "              ('bn2.num_batches_tracked', tensor(407)),\n",
       "              ('fc3.weight',\n",
       "               tensor([[-0.0033,  0.0129, -0.0123,  ..., -0.0393,  0.0080, -0.0310],\n",
       "                       [ 0.0020, -0.0156,  0.0055,  ...,  0.0069,  0.0022, -0.0085],\n",
       "                       [ 0.0087,  0.0162, -0.0021,  ...,  0.0071, -0.0053, -0.0046],\n",
       "                       ...,\n",
       "                       [-0.0125, -0.0045,  0.0088,  ...,  0.0006,  0.0061,  0.0023],\n",
       "                       [ 0.0078, -0.0091,  0.0026,  ..., -0.0094, -0.0090, -0.0156],\n",
       "                       [-0.0098,  0.0014,  0.0075,  ...,  0.0068,  0.0083,  0.0003]])),\n",
       "              ('fc3.bias',\n",
       "               tensor([ 1.5429e-09, -6.4274e-05,  3.2148e-04,  2.3643e-09,  1.0221e-09,\n",
       "                        1.4321e-08, -3.2676e-08, -7.1913e-05,  2.7587e-08,  1.1442e-05,\n",
       "                       -1.2565e-04, -1.7954e-08, -9.6613e-05, -1.0402e-08, -6.7541e-07,\n",
       "                       -5.9128e-09,  8.2251e-09, -2.6094e-08, -4.5012e-09, -5.9769e-07,\n",
       "                        4.7386e-08,  2.7683e-04,  2.4691e-08, -2.4599e-09, -2.9900e-05,\n",
       "                        3.6806e-07, -3.6211e-08,  1.8615e-04, -2.0979e-09,  2.7434e-08,\n",
       "                       -2.4218e-04, -7.8273e-09,  4.6913e-08,  1.3952e-04, -1.7038e-08,\n",
       "                       -8.1924e-10,  3.5653e-09,  2.3348e-09,  2.3259e-08, -3.2574e-08,\n",
       "                       -3.7234e-05,  9.4397e-09, -1.0291e-08,  1.2369e-08,  5.5432e-06,\n",
       "                       -2.5648e-09,  2.3622e-05, -1.9513e-08,  4.1478e-08,  3.2991e-09,\n",
       "                        2.6144e-08,  1.5290e-08, -2.7690e-09, -2.8611e-04, -6.0105e-08,\n",
       "                       -1.0979e-08, -3.2084e-08,  9.2262e-09,  1.2555e-08, -3.8996e-08,\n",
       "                       -1.8739e-06, -8.8004e-08,  1.1711e-09, -4.1353e-07])),\n",
       "              ('bn3.weight',\n",
       "               tensor([0.7962, 0.7964, 0.7944, 0.8068, 0.7960, 0.7997, 0.7972, 0.8172, 0.7966,\n",
       "                       0.7914, 0.8046, 0.7957, 0.7937, 0.8059, 0.8017, 0.8104, 0.8051, 0.8049,\n",
       "                       0.8116, 0.7971, 0.7965, 0.7922, 0.7925, 0.8006, 0.7965, 0.7970, 0.8000,\n",
       "                       0.7974, 0.7972, 0.7998, 0.8079, 0.8006, 0.7992, 0.7988, 0.7936, 0.8011,\n",
       "                       0.8047, 0.7956, 0.8031, 0.7955, 0.7974, 0.7956, 0.7924, 0.8101, 0.7958,\n",
       "                       0.7962, 0.7953, 0.7987, 0.7978, 0.7962, 0.8012, 0.8013, 0.8036, 0.8122,\n",
       "                       0.8013, 0.7932, 0.7973, 0.7919, 0.7981, 0.7965, 0.7946, 0.8034, 0.7926,\n",
       "                       0.8065])),\n",
       "              ('bn3.bias',\n",
       "               tensor([-0.0078, -0.0202, -0.0036, -0.0092, -0.0153, -0.0062, -0.0098, -0.0074,\n",
       "                        0.0032, -0.0088, -0.0109, -0.0171, -0.0110, -0.0116, -0.0087, -0.0093,\n",
       "                       -0.0124, -0.0133, -0.0162, -0.0060, -0.0318, -0.0176, -0.0138, -0.0056,\n",
       "                       -0.0227, -0.0067, -0.0080, -0.0087,  0.0013, -0.0069, -0.0127, -0.0043,\n",
       "                       -0.0116, -0.0084, -0.0125, -0.0067, -0.0104, -0.0071, -0.0084, -0.0176,\n",
       "                       -0.0163, -0.0255, -0.0067, -0.0179, -0.0111, -0.0153, -0.0138, -0.0001,\n",
       "                       -0.0256, -0.0121, -0.0079, -0.0170, -0.0064, -0.0087, -0.0083, -0.0150,\n",
       "                       -0.0068, -0.0172, -0.0186, -0.0225, -0.0171, -0.0104, -0.0097, -0.0118])),\n",
       "              ('bn3.running_mean',\n",
       "               tensor([-0.1596, -0.0657, -0.1571,  0.0299, -0.0515,  0.0039, -0.0467,  0.0093,\n",
       "                        0.0361, -0.1752, -0.0344, -0.0850, -0.0932,  0.0147,  0.0004, -0.0153,\n",
       "                        0.0196,  0.0358, -0.0697, -0.1316, -0.1008, -0.1364, -0.1197, -0.0138,\n",
       "                       -0.0197, -0.1347,  0.0215,  0.0301,  0.0308, -0.1473,  0.0192,  0.0008,\n",
       "                       -0.0649, -0.0088, -0.1198,  0.0130, -0.0119, -0.1409, -0.0039, -0.1363,\n",
       "                       -0.0211, -0.0919, -0.1066,  0.0032, -0.0783, -0.0614, -0.1578,  0.0008,\n",
       "                       -0.0887, -0.0548, -0.1427, -0.0518,  0.0169,  0.0118, -0.0342, -0.1470,\n",
       "                       -0.0063, -0.1606, -0.0514, -0.0677, -0.1299,  0.0196, -0.1652,  0.0109])),\n",
       "              ('bn3.running_var',\n",
       "               tensor([0.0232, 0.0080, 0.0163, 0.0028, 0.0035, 0.0002, 0.0016, 0.0029, 0.0008,\n",
       "                       0.0242, 0.0014, 0.0054, 0.0097, 0.0034, 0.0009, 0.0040, 0.0034, 0.0033,\n",
       "                       0.0228, 0.0123, 0.0064, 0.0161, 0.0141, 0.0023, 0.0024, 0.0194, 0.0012,\n",
       "                       0.0015, 0.0013, 0.0141, 0.0054, 0.0007, 0.0028, 0.0017, 0.0089, 0.0020,\n",
       "                       0.0016, 0.0249, 0.0008, 0.0086, 0.0023, 0.0089, 0.0092, 0.0062, 0.0032,\n",
       "                       0.0050, 0.0241, 0.0042, 0.0065, 0.0042, 0.0282, 0.0030, 0.0030, 0.0032,\n",
       "                       0.0020, 0.0190, 0.0014, 0.0228, 0.0020, 0.0039, 0.0143, 0.0025, 0.0224,\n",
       "                       0.0027])),\n",
       "              ('bn3.num_batches_tracked', tensor(407)),\n",
       "              ('fc4.weight',\n",
       "               tensor([[-0.0169, -0.0719,  0.0480,  ..., -0.0473,  0.0367,  0.0099],\n",
       "                       [-0.0024, -0.0471, -0.0554,  ..., -0.0408,  0.0832,  0.0528],\n",
       "                       [ 0.0067,  0.0662,  0.0109,  ..., -0.0141, -0.0374, -0.0007],\n",
       "                       ...,\n",
       "                       [ 0.0652,  0.0356,  0.0502,  ..., -0.0132,  0.0447, -0.0452],\n",
       "                       [-0.0769,  0.0277, -0.0451,  ...,  0.0571,  0.0217, -0.0158],\n",
       "                       [ 0.0152, -0.0519,  0.0437,  ..., -0.0141, -0.0073, -0.0500]])),\n",
       "              ('fc4.bias',\n",
       "               tensor([-0.0745,  0.0343, -0.0452, -0.0412, -0.1114, -0.0666, -0.0408, -0.0729,\n",
       "                        0.0572, -0.0885, -0.0822, -0.0084, -0.1111,  0.0021,  0.0365,  0.0563,\n",
       "                       -0.0716, -0.0048, -0.0527, -0.0300,  0.1116,  0.0025,  0.0210,  0.0467,\n",
       "                       -0.1220,  0.1200,  0.0137, -0.0113,  0.0010,  0.0797,  0.0361,  0.0441])),\n",
       "              ('fc5.weight',\n",
       "               tensor([[ 0.1104, -0.1285, -0.0940, -0.0422,  0.0888,  0.0482, -0.0166,  0.0635,\n",
       "                        -0.1495,  0.1090,  0.0595,  0.0201,  0.0745, -0.0958, -0.0218,  0.0088,\n",
       "                         0.1005,  0.0493,  0.1236, -0.1062, -0.1203,  0.0047, -0.0152, -0.1014,\n",
       "                         0.1666, -0.1771, -0.1088,  0.0342, -0.1199, -0.1625,  0.1383,  0.0636]])),\n",
       "              ('fc5.bias', tensor([-0.0676]))]),\n",
       " [0.5369172692298889, 0.5602972507476807, 0.5392981171607971])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Perform cross-validation\n",
    "cross_val_train(RegressionNN, X_train, y_train, epochs=500, k_folds=3, patience=50)\n",
    "# Save the model\n",
    "# Example usage: torch.save(model.state_dict(), \"enhanced_regression_model.pth\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Antonia\\AppData\\Local\\Temp\\ipykernel_9536\\3257265039.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], RMSE: 0.5015\n",
      "Epoch [200/1000], RMSE: 0.4643\n",
      "Epoch [300/1000], RMSE: 0.4415\n",
      "Epoch [400/1000], RMSE: 0.4311\n",
      "Early stopping triggered at epoch 452\n",
      "Training on full dataset completed.\n",
      "\n",
      "Test RMSE: 0.5367\n",
      "Final model saved as 'final_model.pth'.\n"
     ]
    }
   ],
   "source": [
    "# Load the best model and train it on the full training set\n",
    "def train_on_full_data(model_class, X_train, y_train, X_test, y_test, criterion, epochs=100,  patience=80):\n",
    "    model = model_class(X_train.shape[1])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20, verbose=True)\n",
    "    # Load the best model state\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_test)\n",
    "            val_loss = criterion(val_outputs, y_test)\n",
    "\n",
    "\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "            break\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            rmse = torch.sqrt(loss).item()\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}], RMSE: {rmse:.4f}\")\n",
    "\n",
    "    print(\"Training on full dataset completed.\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test)\n",
    "        test_loss = criterion(test_outputs, y_test)\n",
    "        test_rmse = torch.sqrt(test_loss).item()\n",
    "        print(f\"\\nTest RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "    # Save the final model\n",
    "    torch.save(model.state_dict(), \"final_model.pth\")\n",
    "    print(\"Final model saved as 'final_model.pth'.\")\n",
    "\n",
    "# Train the best model on the full training set and evaluate on test set\n",
    "train_on_full_data(RegressionNN, X_train, y_train, X_test, y_test, criterion, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0.11333410441875458]\n",
    "[0.11041968315839767]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MDMLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
